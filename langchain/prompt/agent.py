from langchain.prompts import (
    PromptTemplate,
    StringPromptTemplate,
    FewShotPromptTemplate,
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    AIMessagePromptTemplate,
)
from langchain_core.prompts.pipeline import PipelinePromptTemplate
from langchain_core.prompts.structured import StructuredPrompt
from langchain_core.prompts.image import ImagePromptTemplate
from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain.callbacks import StdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler
from typing import List, Dict, Any
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class CustomCallbackHandler(BaseCallbackHandler):
    """è‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨ï¼Œç”¨äºç›‘æ§Agentæ‰§è¡Œè¿‡ç¨‹"""
    
    def on_llm_start(self, *args, **kwargs):
        """å½“LLMå¼€å§‹ç”Ÿæˆæ—¶è§¦å‘"""
        print("ğŸ¤– LLMå¼€å§‹ç”Ÿæˆ...")
    
    def on_llm_end(self, *args, **kwargs):
        """å½“LLMå®Œæˆç”Ÿæˆæ—¶è§¦å‘"""
        print("âœ… LLMç”Ÿæˆå®Œæˆ")
    
    def on_tool_start(self, *args, **kwargs):
        """å½“å·¥å…·å¼€å§‹æ‰§è¡Œæ—¶è§¦å‘"""
        print("ğŸ”§ å·¥å…·å¼€å§‹æ‰§è¡Œ...")
    
    def on_tool_end(self, *args, **kwargs):
        """å½“å·¥å…·æ‰§è¡Œå®Œæˆæ—¶è§¦å‘"""
        print("âœ… å·¥å…·æ‰§è¡Œå®Œæˆ")
    
    def on_chain_start(self, *args, **kwargs):
        """å½“é“¾å¼€å§‹æ‰§è¡Œæ—¶è§¦å‘"""
        print("â›“ï¸ é“¾å¼€å§‹æ‰§è¡Œ...")
    
    def on_chain_end(self, *args, **kwargs):
        """å½“é“¾æ‰§è¡Œå®Œæˆæ—¶è§¦å‘"""
        print("âœ… é“¾æ‰§è¡Œå®Œæˆ")
    
    def on_agent_action(self, *args, **kwargs):
        """å½“Agenté€‰æ‹©è¡ŒåŠ¨æ—¶è§¦å‘"""
        print("ğŸ¤” Agentæ­£åœ¨æ€è€ƒä¸‹ä¸€æ­¥è¡ŒåŠ¨...")

# åˆå§‹åŒ– LLM
def get_llm():
    return ChatOpenAI(
        model="deepseek-chat",
        temperature=0.7,
        callbacks=[StdOutCallbackHandler(), CustomCallbackHandler()]
    )

# 1. PromptTemplate - åŸºç¡€æ¨¡æ¿
def basic_prompt_template_example():
    """åŸºç¡€æç¤ºæ¨¡æ¿ç¤ºä¾‹"""
    llm = get_llm()
    
    # åˆ›å»ºäº§å“ä»‹ç»æ¨¡æ¿
    # å®šä¹‰äº†ä¸‰ä¸ªè¾“å…¥å˜é‡ï¼šproduct_name, features, price
    # æ¨¡æ¿ç»“æ„æ¸…æ™°ï¼ŒåŒ…å«äº†å…·ä½“çš„è¾“å‡ºè¦æ±‚
    product_prompt = PromptTemplate(
        input_variables=["product_name", "features", "price"],
        template="""è¯·ä¸ºä»¥ä¸‹äº§å“ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„ä»‹ç»ï¼š

äº§å“åç§°ï¼š{product_name}
äº§å“ç‰¹ç‚¹ï¼š{features}
ä»·æ ¼ï¼š{price}

è¯·ç”¨3-5å¥è¯ä»‹ç»è¿™ä¸ªäº§å“çš„ä¸»è¦ä¼˜åŠ¿å’Œä»·å€¼ã€‚"""
    )
    
    # ä½¿ç”¨formatæ–¹æ³•å¡«å……æ¨¡æ¿å˜é‡
    prompt = product_prompt.format(
        product_name="æ™ºèƒ½æ‰‹è¡¨ Mini",
        features="é˜²æ°´ã€å¿ƒç‡ç›‘æµ‹ã€è¿åŠ¨è¿½è¸ª",
        price="599å…ƒ"
    )
    
    # è°ƒç”¨LLMç”Ÿæˆå“åº”
    response = llm.invoke(prompt)
    return response.content

# 2. StringPromptTemplateç¤ºä¾‹
def string_prompt_template_example():
    """å­—ç¬¦ä¸²æç¤ºæ¨¡æ¿ç¤ºä¾‹"""
    llm = get_llm()
    
    # åˆ›å»ºè‡ªå®šä¹‰çš„StringPromptTemplate
    class TranslationPromptTemplate(StringPromptTemplate):
        # ä½¿ç”¨ç±»å‹æ³¨è§£å®šä¹‰è¾“å…¥å˜é‡
        input_variables: List[str] = ["text", "target_language", "formality_level"]
        
        def format(self, **kwargs) -> str:
            # è‡ªåŠ¨è·å–ç³»ç»Ÿæ—¶é—´
            from datetime import datetime
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # è‡ªåŠ¨æ£€æµ‹æºè¯­è¨€ï¼ˆè¿™é‡Œç”¨ç®€å•é€»è¾‘æ¼”ç¤ºï¼‰
            def detect_language(text):
                import re
                # ç®€å•åˆ¤æ–­ï¼šå¦‚æœåŒ…å«ä¸­æ–‡å­—ç¬¦åˆ™è®¤ä¸ºæ˜¯ä¸­æ–‡
                if re.search(r'[\u4e00-\u9fff]', text):
                    return "ä¸­æ–‡"
                return "è‹±æ–‡"
            
            # æ‰©å±•å˜é‡
            kwargs["timestamp"] = current_time
            kwargs["source_language"] = detect_language(kwargs["text"])
            
            # ä½¿ç”¨æ¨¡æ¿å­—ç¬¦ä¸²
            template = """[ç³»ç»Ÿæ—¶é—´: {timestamp}]

è¯·å°†ä»¥ä¸‹æ–‡æœ¬ä»{source_language}ç¿»è¯‘æˆ{target_language}ï¼š

åŸæ–‡ï¼š{text}

ç¿»è¯‘è¦æ±‚ï¼š
- è¯­è¨€é£æ ¼ï¼š{formality_level}
- ä¿æŒåŸæ–‡çš„è¯­æ°”å’Œé£æ ¼
- ç¡®ä¿ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§
- é€‚åº”ç›®æ ‡è¯­è¨€çš„æ–‡åŒ–èƒŒæ™¯"""
            
            return template.format(**kwargs)
    
    # åˆ›å»ºç¿»è¯‘æ¨¡æ¿å®ä¾‹
    translation_prompt = TranslationPromptTemplate()
    
    # æ ¼å¼åŒ–æç¤ºæ¨¡æ¿ï¼ˆåªéœ€æä¾›å¿…è¦å‚æ•°ï¼Œå…¶ä»–å‚æ•°ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
    prompt = translation_prompt.format(
        text="äººå·¥æ™ºèƒ½æ­£åœ¨é‡å¡‘æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚",
        target_language="è‹±è¯­",
        formality_level="formal"
    )
    
    # è°ƒç”¨LLMç”Ÿæˆç¿»è¯‘
    response = llm.invoke(prompt)
    return response.content

# 3. ç»“æ„åŒ–æç¤ºæ¨¡æ¿ç¤ºä¾‹
def structured_prompt_template_example():
    """ç»“æ„åŒ–æç¤ºæ¨¡æ¿ç¤ºä¾‹"""
    llm = get_llm()
    
    # åˆ›å»ºç»“æ„åŒ–æç¤ºæ¨¡æ¿
    # å®šä¹‰äº§å“åˆ†æçš„ç»“æ„åŒ–è¾“å…¥æ¨¡å¼ï¼ŒåŒ…å«å¤šå±‚åµŒå¥—ç»“æ„
    product_analysis = StructuredPrompt(
        name="product_analysis",
        template="""è¯·å¯¹ä»¥ä¸‹äº§å“è¿›è¡Œå…¨é¢åˆ†æï¼š

äº§å“åŸºæœ¬ä¿¡æ¯ï¼š
- äº§å“åç§°ï¼š{product_info[name]}
- äº§å“ä»£å·ï¼š{product_info[code]}
- äº§å“ç‰ˆæœ¬ï¼š{product_info[version]}

å¸‚åœºåˆ†æï¼š
1. ç›®æ ‡å¸‚åœº
   - ä¸»è¦ç”¨æˆ·ç¾¤ï¼š{market_analysis[target_market][user_groups]}
   - å¸‚åœºè§„æ¨¡ï¼š{market_analysis[target_market][market_size]}
   - åœ°åŸŸåˆ†å¸ƒï¼š{market_analysis[target_market][regions]}

2. ç«å“åˆ†æ
   - ä¸»è¦ç«å“ï¼š{market_analysis[competitors][main_competitors]}
   - ç«äº‰ä¼˜åŠ¿ï¼š{market_analysis[competitors][advantages]}
   - ç«äº‰åŠ£åŠ¿ï¼š{market_analysis[competitors][disadvantages]}

3. ä»·æ ¼ç­–ç•¥
   - å»ºè®®é›¶å”®ä»·ï¼š{market_analysis[pricing][retail_price]}
   - æˆæœ¬ç»“æ„ï¼š{market_analysis[pricing][cost_structure]}
   - åˆ©æ¶¦ç©ºé—´ï¼š{market_analysis[pricing][profit_margin]}

æŠ€æœ¯åˆ†æï¼š
1. åŠŸèƒ½ç‰¹æ€§
   - æ ¸å¿ƒåŠŸèƒ½ï¼š{tech_analysis[features][core_features]}
   - åˆ›æ–°ç‚¹ï¼š{tech_analysis[features][innovations]}
   - æ‰©å±•æ€§ï¼š{tech_analysis[features][scalability]}

2. æ€§èƒ½æŒ‡æ ‡
   - æ€§èƒ½å‚æ•°ï¼š{tech_analysis[performance][parameters]}
   - ç¨³å®šæ€§ï¼š{tech_analysis[performance][stability]}
   - å…¼å®¹æ€§ï¼š{tech_analysis[performance][compatibility]}

3. æŠ€æœ¯æ¶æ„
   - ç³»ç»Ÿæ¶æ„ï¼š{tech_analysis[architecture][system]}
   - å…³é”®æŠ€æœ¯ï¼š{tech_analysis[architecture][key_technologies]}
   - æŠ€æœ¯éš¾ç‚¹ï¼š{tech_analysis[architecture][challenges]}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯æä¾›ï¼š
1. äº§å“å®šä½åˆ†æ
2. å¸‚åœºæœºä¼šè¯„ä¼°
3. æŠ€æœ¯å¯è¡Œæ€§åˆ†æ
4. æŠ•èµ„é£é™©è¯„ä¼°
5. å‘å±•å»ºè®®""",
        input_schema={
            "product_info": {
                "name": str,
                "code": str,
                "version": str
            },
            "market_analysis": {
                "target_market": {
                    "user_groups": str,
                    "market_size": str,
                    "regions": str
                },
                "competitors": {
                    "main_competitors": str,
                    "advantages": str,
                    "disadvantages": str
                },
                "pricing": {
                    "retail_price": str,
                    "cost_structure": str,
                    "profit_margin": str
                }
            },
            "tech_analysis": {
                "features": {
                    "core_features": str,
                    "innovations": str,
                    "scalability": str
                },
                "performance": {
                    "parameters": str,
                    "stability": str,
                    "compatibility": str
                },
                "architecture": {
                    "system": str,
                    "key_technologies": str,
                    "challenges": str
                }
            }
        }
    )
    
    # ä½¿ç”¨ç»“æ„åŒ–æç¤ºæ¨¡æ¿ç”Ÿæˆåˆ†æ
    response = llm.invoke(
        product_analysis.format(
            product_info={
                "name": "æ™ºèƒ½æ‰‹è¡¨Pro Max",
                "code": "SW-2024",
                "version": "2.0"
            },
            market_analysis={
                "target_market": {
                    "user_groups": "é«˜ç«¯å•†åŠ¡äººå£«ã€è¿åŠ¨çˆ±å¥½è€…ã€å¥åº·ç®¡ç†äººç¾¤",
                    "market_size": "é¢„è®¡2024å¹´è¾¾åˆ°500äº¿å…ƒ",
                    "regions": "ä¸€çº¿åŠæ–°ä¸€çº¿åŸå¸‚ä¸ºä¸»"
                },
                "competitors": {
                    "main_competitors": "Apple Watchã€åä¸ºWatchã€å°ç±³æ‰‹ç¯",
                    "advantages": "æ›´é•¿ç»­èˆªæ—¶é—´ã€æ›´å…¨é¢çš„å¥åº·ç›‘æµ‹ã€æ›´ä¼˜æ€§ä»·æ¯”",
                    "disadvantages": "å“ç‰ŒçŸ¥ååº¦è¾ƒä½ã€ç”Ÿæ€ç³»ç»Ÿä¸å¤Ÿå®Œå–„"
                },
                "pricing": {
                    "retail_price": "1999-2999å…ƒ",
                    "cost_structure": "ç¡¬ä»¶æˆæœ¬65%ã€ç ”å‘æˆæœ¬20%ã€è¥é”€æˆæœ¬15%",
                    "profit_margin": "æ¯›åˆ©ç‡35%-40%"
                }
            },
            tech_analysis={
                "features": {
                    "core_features": "å¿ƒç‡ç›‘æµ‹ã€è¡€æ°§æ£€æµ‹ã€è¿åŠ¨è¿½è¸ªã€ç¡çœ åˆ†æ",
                    "innovations": "AIå¥åº·åŠ©æ‰‹ã€æ™ºèƒ½è¿åŠ¨è§„åˆ’ã€æƒ…ç»ªç®¡ç†",
                    "scalability": "æ”¯æŒç¬¬ä¸‰æ–¹åº”ç”¨å¼€å‘ã€å¯æ‰©å±•ä¼ æ„Ÿå™¨æ¨¡å—"
                },
                "performance": {
                    "parameters": "ç»­èˆª15å¤©ã€é˜²æ°´50ç±³ã€å¿ƒç‡è¯¯å·®<3%",
                    "stability": "MTBF>5000å°æ—¶ã€ç³»ç»Ÿç¨³å®šæ€§99.9%",
                    "compatibility": "æ”¯æŒiOS/Androidã€è“ç‰™5.0ã€NFC"
                },
                "architecture": {
                    "system": "è‡ªç ”RTOSç³»ç»Ÿã€åŒæ ¸å¤„ç†å™¨ã€ç‹¬ç«‹AIåå¤„ç†å™¨",
                    "key_technologies": "ä½åŠŸè€—è“ç‰™ã€AIç®—æ³•ã€ä¼ æ„Ÿå™¨èåˆ",
                    "challenges": "ç»­èˆªä¸åŠŸèƒ½å¹³è¡¡ã€æ•°æ®ç²¾åº¦æå‡ã€æ•£çƒ­ä¼˜åŒ–"
                }
            }
        )
    )
    return response.content

# 4. PipelinePromptTemplate - ç®¡é“æ¨¡æ¿
def pipeline_prompt_template_example():
    """ç®¡é“æç¤ºæ¨¡æ¿ç¤ºä¾‹"""
    llm = get_llm()
    
    # åˆ›å»ºå¤§çº²ç”Ÿæˆæ¨¡æ¿
    # ç¬¬ä¸€é˜¶æ®µï¼šæ ¹æ®ä¸»é¢˜å’Œè¦æ±‚ç”Ÿæˆæ•™å­¦å¤§çº²
    outline_prompt = PromptTemplate(
        input_variables=["topic", "education_level", "learning_objectives"],
        template="""è¯·ä¸ºä»¥ä¸‹æ•™è‚²å†…å®¹åˆ›å»ºè¯¦ç»†å¤§çº²ï¼š

ä¸»é¢˜ï¼š{topic}
æ•™è‚²æ°´å¹³ï¼š{education_level}
å­¦ä¹ ç›®æ ‡ï¼š{learning_objectives}

å¤§çº²è¦æ±‚ï¼š
1. ç¬¦åˆæ•™è‚²æ°´å¹³çš„è®¤çŸ¥èƒ½åŠ›
2. å¾ªåºæ¸è¿›çš„çŸ¥è¯†ç»“æ„
3. åŒ…å«äº’åŠ¨å’Œå®è·µç¯èŠ‚"""
    )
    
    # åˆ›å»ºå†…å®¹ç¼–å†™æ¨¡æ¿
    # ç¬¬äºŒé˜¶æ®µï¼šåŸºäºå¤§çº²ç”Ÿæˆè¯¦ç»†å†…å®¹
    content_prompt = PromptTemplate(
        input_variables=["outline", "teaching_style", "examples"],
        template="""åŸºäºä»¥ä¸‹å¤§çº²ï¼Œåˆ›å»ºè¯¦ç»†çš„æ•™å­¦å†…å®¹ï¼š

å¤§çº²å†…å®¹ï¼š
{outline}

æ•™å­¦é£æ ¼ï¼š{teaching_style}
å®ä¾‹è¦æ±‚ï¼š{examples}"""
    )
    
    # åˆ›å»ºç»ƒä¹ é¢˜ç”Ÿæˆæ¨¡æ¿
    # ç¬¬ä¸‰é˜¶æ®µï¼šæ ¹æ®å†…å®¹ç”Ÿæˆç»ƒä¹ é¢˜
    exercise_prompt = PromptTemplate(
        input_variables=["content", "difficulty_level", "question_types"],
        template="""åŸºäºä»¥ä¸‹æ•™å­¦å†…å®¹ï¼Œç”Ÿæˆç»ƒä¹ é¢˜ï¼š

è¯¾ç¨‹å†…å®¹ï¼š
{content}

éš¾åº¦ç­‰çº§ï¼š{difficulty_level}
é¢˜å‹è¦æ±‚ï¼š{question_types}"""
    )
    
    # åˆ›å»ºç®¡é“æ¨¡æ¿
    # å°†ä¸‰ä¸ªæ¨¡æ¿ä¸²è”èµ·æ¥ï¼Œå½¢æˆå®Œæ•´çš„æ•™å­¦å†…å®¹ç”Ÿæˆæµç¨‹
    pipeline_prompt = PipelinePromptTemplate(
        final_prompt=exercise_prompt,
        pipeline_prompts=[
            ("outline", outline_prompt),
            ("content", content_prompt)
        ]
    )
    
    # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå¤§çº²
    outline_result = llm.invoke(outline_prompt.format(
        topic="PythonåŸºç¡€ç¼–ç¨‹ï¼šå¾ªç¯ç»“æ„",
        education_level="é«˜ä¸­ä¿¡æ¯æŠ€æœ¯",
        learning_objectives="ç†è§£å’ŒæŒæ¡Pythonä¸­çš„forå’Œwhileå¾ªç¯ä½¿ç”¨"
    ))
    
    # ç¬¬äºŒæ­¥ï¼šåŸºäºå¤§çº²ç”Ÿæˆå†…å®¹
    content_result = llm.invoke(content_prompt.format(
        outline=outline_result.content,
        teaching_style="äº’åŠ¨å¼ï¼Œä»¥å®ä¾‹ä¸ºå¯¼å‘",
        examples="æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¾ªç¯æ¡ˆä¾‹"
    ))
    
    # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç»ƒä¹ é¢˜
    final_result = llm.invoke(exercise_prompt.format(
        content=content_result.content,
        difficulty_level="ä¸­ç­‰",
        question_types="é€‰æ‹©é¢˜ã€ç¼–ç¨‹é¢˜ã€åº”ç”¨é¢˜"
    ))
    
    return {
        "outline": outline_result.content,
        "content": content_result.content,
        "exercises": final_result.content
    }

# 5. FewShotPromptTemplate - å°‘æ ·æœ¬æ¨¡æ¿
def few_shot_prompt_template_example():
    """å°‘æ ·æœ¬æç¤ºæ¨¡æ¿ç¤ºä¾‹
    
    FewShotPromptTemplateæ˜¯ä¸€ç§åŸºäºç¤ºä¾‹å­¦ä¹ çš„æ¨¡æ¿ï¼Œé€šè¿‡æä¾›å°‘é‡ç¤ºä¾‹æ¥æŒ‡å¯¼æ¨¡å‹ç†è§£ä»»åŠ¡ã€‚
    è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚åˆéœ€è¦é€šè¿‡å…·ä½“ä¾‹å­æ¥è¯´æ˜ä»»åŠ¡è¦æ±‚çš„åœºæ™¯ã€‚
    
    ä¸»è¦ç‰¹ç‚¹ï¼š
    1. æ”¯æŒç¤ºä¾‹å­¦ä¹ ï¼Œé€šè¿‡å…·ä½“ä¾‹å­å¼•å¯¼æ¨¡å‹
    2. å¯ä»¥è®¾ç½®ç¤ºä¾‹çš„å±•ç¤ºæ ¼å¼
    3. é€‚åˆåˆ†ç±»ã€æƒ…æ„Ÿåˆ†æç­‰éœ€è¦å‚è€ƒæ ·æœ¬çš„ä»»åŠ¡
    """
    llm = get_llm()
    
    # åˆ›å»ºç¤ºä¾‹
    # æä¾›äº†ä¸‰ä¸ªä¸åŒæƒ…æ„Ÿå€¾å‘çš„æ–‡æœ¬æ ·æœ¬
    examples = [
        {"text": "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½ï¼Œå¾ˆè€ç”¨", "sentiment": "æ­£é¢"},
        {"text": "ä»·æ ¼å¤ªè´µäº†ï¼Œä¸å¤ªåˆ’ç®—", "sentiment": "è´Ÿé¢"},
        {"text": "ä¸€èˆ¬èˆ¬ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„", "sentiment": "ä¸­æ€§"}
    ]
    
    # åˆ›å»ºç¤ºä¾‹æ¨¡æ¿
    # å®šä¹‰äº†å¦‚ä½•å±•ç¤ºæ¯ä¸ªç¤ºä¾‹
    example_prompt = PromptTemplate(
        input_variables=["text", "sentiment"],
        template="æ–‡æœ¬: {text}\næƒ…æ„Ÿ: {sentiment}"
    )
    
    # åˆ›å»ºå°‘æ ·æœ¬æ¨¡æ¿
    # ç»„åˆç¤ºä¾‹å’Œæ¨¡æ¿ï¼Œæ„å»ºå®Œæ•´çš„å°‘æ ·æœ¬å­¦ä¹ ç»“æ„
    few_shot_prompt = FewShotPromptTemplate(
        examples=examples,
        example_prompt=example_prompt,
        prefix="ä»¥ä¸‹æ˜¯ä¸€äº›æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»çš„ä¾‹å­ï¼š\n\n",
        suffix="\n\næ–‡æœ¬: {input_text}\næƒ…æ„Ÿ:",
        input_variables=["input_text"]
    )
    
    # æ ¼å¼åŒ–æç¤º
    prompt = few_shot_prompt.format(
        input_text="è¿™ä¸ªå•†å“çš„åŒ…è£…å¾ˆç²¾ç¾ï¼Œä½†æ˜¯å‘è´§å¤ªæ…¢äº†"
    )
    
    # è°ƒç”¨LLMè¿›è¡Œæƒ…æ„Ÿåˆ†æ
    response = llm.invoke(prompt)
    return response.content

# 6. ChatPromptTemplate - å¯¹è¯æ¨¡æ¿
def chat_prompt_template_example():
    """å¯¹è¯æç¤ºæ¨¡æ¿ç¤ºä¾‹
    
    ChatPromptTemplateä¸“é—¨ç”¨äºæ„å»ºå¯¹è¯ç³»ç»Ÿï¼Œå®ƒå¯ä»¥ç»„åˆå¤šä¸ªè§’è‰²çš„æ¶ˆæ¯ï¼Œ
    åˆ›å»ºè‡ªç„¶æµç•…çš„å¯¹è¯äº¤äº’ã€‚
    
    ä¸»è¦ç‰¹ç‚¹ï¼š
    1. æ”¯æŒå¤šè§’è‰²å¯¹è¯
    2. å¯ä»¥å®šåˆ¶å¯¹è¯é£æ ¼å’Œè¯­æ°”
    3. é€‚åˆå®¢æœã€å’¨è¯¢ç­‰å¯¹è¯åœºæ™¯
    """
    llm = get_llm()
    
    # åˆ›å»ºæ¶ˆæ¯æ¨¡æ¿
    # åˆ†åˆ«å®šä¹‰ç³»ç»Ÿã€ç”¨æˆ·å’ŒAIçš„æ¶ˆæ¯æ¨¡æ¿
    system_template = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}ï¼Œä¸“é—¨è§£ç­”å…³äº{product}çš„é—®é¢˜ã€‚è¯·ä½¿ç”¨{tone}çš„è¯­æ°”ã€‚"
    human_template = "{question}"
    ai_template = "æˆ‘ç†è§£æ‚¨çš„é—®é¢˜æ˜¯å…³äº{product}çš„{question_type}ã€‚è®©æˆ‘ä¸ºæ‚¨è¯¦ç»†è§£ç­”ã€‚"
    
    # åˆ›å»ºæ¶ˆæ¯æç¤ºæ¨¡æ¿
    # å°†ä¸åŒè§’è‰²çš„æ¨¡æ¿è½¬æ¢ä¸ºå¯¹åº”çš„æ¶ˆæ¯ç±»å‹
    system_message = SystemMessagePromptTemplate.from_template(system_template)
    human_message = HumanMessagePromptTemplate.from_template(human_template)
    ai_message = AIMessagePromptTemplate.from_template(ai_template)
    
    # åˆ›å»ºå¯¹è¯æ¨¡æ¿
    # æŒ‰ç…§å¯¹è¯æµç¨‹ç»„åˆå„ä¸ªæ¶ˆæ¯
    chat_prompt = ChatPromptTemplate.from_messages([
        system_message,
        human_message,
        ai_message
    ])
    
    # æ ¼å¼åŒ–æ¶ˆæ¯
    # å¡«å……æ¨¡æ¿å˜é‡ï¼Œç”Ÿæˆå®Œæ•´çš„å¯¹è¯å†…å®¹
    messages = chat_prompt.format_messages(
        role="æŠ€æœ¯æ”¯æŒä¸“å®¶",
        product="æ™ºèƒ½æ‰‹æœº",
        tone="ä¸“ä¸šå‹å¥½",
        question="å¦‚ä½•è§£å†³ç”µæ± ç»­èˆªé—®é¢˜ï¼Ÿ",
        question_type="æŠ€æœ¯æ”¯æŒ"
    )
    
    # è°ƒç”¨LLMç”Ÿæˆå›ç­”
    response = llm.invoke(messages)
    return response.content

# 7. ImagePromptTemplate - å›¾åƒæç¤ºæ¨¡æ¿
def image_prompt_template_example():
    """å›¾åƒæç¤ºæ¨¡æ¿ç¤ºä¾‹
    
    ImagePromptTemplateç”¨äºå¤„ç†å›¾åƒç›¸å…³çš„ä»»åŠ¡ï¼Œå®ƒå¯ä»¥ç»“åˆå›¾åƒURLå’Œå…¶ä»–å‚æ•°ï¼Œ
    æ„å»ºé€‚åˆå›¾åƒåˆ†æã€æè¿°ç­‰ä»»åŠ¡çš„æç¤ºã€‚
    
    ä¸»è¦ç‰¹ç‚¹ï¼š
    1. æ”¯æŒå›¾åƒURLå’Œè¯¦ç»†åº¦å‚æ•°
    2. å¯ä»¥ä¸æ–‡æœ¬æç¤ºç»“åˆ
    3. é€‚åˆå›¾åƒåˆ†æã€æè¿°ç”Ÿæˆç­‰ä»»åŠ¡
    """
    llm = get_llm()
    
    # åˆ›å»ºå›¾åƒå¤„ç†æ¨¡æ¿
    # å®šä¹‰å›¾åƒURLå’Œè¯¦ç»†åº¦å‚æ•°
    image_prompt = ImagePromptTemplate(
        template={"url": "{url}", "detail": "{detail}"},
        template_format="f-string"
    )
    
    # æ ¼å¼åŒ–æç¤º
    # è®¾ç½®å›¾åƒURLå’ŒæœŸæœ›çš„åˆ†æè¯¦ç»†åº¦
    prompt = image_prompt.format(
        url="https://example.com/sunset.jpg",
        detail="high"
    )
    
    # åˆ›å»ºåˆ†ææç¤º
    # å°†å›¾åƒæ•°æ®ä¸åˆ†æè¦æ±‚ç»“åˆ
    analysis_prompt = PromptTemplate(
        input_variables=["image_data"],
        template="è¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹å’Œé£æ ¼ç‰¹ç‚¹ï¼š\n{image_data}"
    )
    
    # è°ƒç”¨LLMç”Ÿæˆå›¾åƒåˆ†æ
    response = llm.invoke(analysis_prompt.format(image_data=str(prompt)))
    return response.content

if __name__ == "__main__":
    print("\n1. åŸºç¡€æç¤ºæ¨¡æ¿ç¤ºä¾‹ï¼š")
    print(basic_prompt_template_example())
    
    print("\n2. è‡ªå®šä¹‰å­—ç¬¦ä¸²æç¤ºæ¨¡æ¿ç¤ºä¾‹ï¼š")
    print(string_prompt_template_example())
    
    print("\n3. ç»“æ„åŒ–æç¤ºæ¨¡æ¿ç¤ºä¾‹ï¼š")
    print(structured_prompt_template_example())
    
    print("\n4. ç®¡é“æç¤ºæ¨¡æ¿ç¤ºä¾‹ï¼š")
    result = pipeline_prompt_template_example()
    print("\nå¤§çº²ï¼š")
    print(result["outline"])
    print("\nå†…å®¹ï¼š")
    print(result["content"])
    print("\nç»ƒä¹ é¢˜ï¼š")
    print(result["exercises"])
    
    print("\n5. å°‘æ ·æœ¬æç¤ºæ¨¡æ¿ç¤ºä¾‹ï¼š")
    print(few_shot_prompt_template_example())
    
    print("\n6. å¯¹è¯æç¤ºæ¨¡æ¿ç¤ºä¾‹ï¼š")
    print(chat_prompt_template_example())
    
    print("\n7. å›¾åƒæç¤ºæ¨¡æ¿ç¤ºä¾‹ï¼š")
    print(image_prompt_template_example())