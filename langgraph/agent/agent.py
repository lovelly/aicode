from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Dict, Any, Optional
from langchain_core.messages import HumanMessage, AIMessage
import langchain_core.tools as tools
from langchain_openai import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import os
from dotenv import load_dotenv
from IPython.display import Image, display
import time
from datetime import datetime
import json
from langchain_core.output_parsers import JsonOutputParser
from langchain.schema import OutputParserException
from pydantic import BaseModel, Field
from bs4 import BeautifulSoup
import re
from collections import Counter
import jieba
import requests
import pandas as pd
import matplotlib.pyplot as plt
import io
import subprocess

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
print(os.getenv("MODEL"))

class CustomCallbackHandler(BaseCallbackHandler):
    """å¢å¼ºç‰ˆå›è°ƒå¤„ç†å™¨ï¼Œæä¾›è¯¦ç»†æ‰§è¡Œç›‘æ§å’Œé”™è¯¯å¤„ç†"""
    
    def __init__(self):
        self.execution_stack = []
        self.timers = {}

    def on_llm_start(self, serialized, prompts, **kwargs):
        self._start_timer('llm')
        print(f"ğŸ•’ [{self._get_timestamp()}] LLMå¼€å§‹ç”Ÿæˆ | æç¤ºæ•°é‡: {len(prompts)}")

    def on_llm_end(self, response, **kwargs):
        duration = self._end_timer('llm')
        print(f"âœ… [{self._get_timestamp()}] LLMç”Ÿæˆå®Œæˆ | è€—æ—¶: {duration:.2f}s | å“åº”é•¿åº¦: {len(str(response))}")

    def on_tool_start(self, serialized, input_str, **kwargs):
        self._start_timer('tool')
        print(f"ğŸ”§ [{self._get_timestamp()}] å·¥å…·å¼€å§‹æ‰§è¡Œ | å·¥å…·: {serialized.get('name')} | è¾“å…¥: {input_str[:200]}")

    def on_tool_end(self, output, **kwargs):
        duration = self._end_timer('tool')
        print(f"âœ… [{self._get_timestamp()}] å·¥å…·æ‰§è¡Œå®Œæˆ | è€—æ—¶: {duration:.2f}s | è¾“å‡º: {str(output)[:200]}")

    def on_tool_error(self, error, **kwargs):
        print(f"âŒ [{self._get_timestamp()}] å·¥å…·æ‰§è¡Œé”™è¯¯ | é”™è¯¯ç±»å‹: {type(error).__name__} | è¯¦æƒ…: {str(error)[:500]}")

    def on_chain_start(self, serialized, inputs, **kwargs):
        self._start_timer('chain')
        chain_type = serialized.get('name', 'æœªçŸ¥é“¾')
        print(f"â›“ï¸ [{self._get_timestamp()}] é“¾å¼€å§‹æ‰§è¡Œ | ç±»å‹: {chain_type} | è¾“å…¥å‚æ•°: {self._format_inputs(inputs)}")

    def on_chain_end(self, outputs, **kwargs):
        duration = self._end_timer('chain')
        print(f"âœ… [{self._get_timestamp()}] é“¾æ‰§è¡Œå®Œæˆ | è€—æ—¶: {duration:.2f}s | è¾“å‡ºå‚æ•°: {self._format_outputs(outputs)}")

    def on_agent_action(self, action, **kwargs):
        print(f"ğŸ¤” [{self._get_timestamp()}] Agentå†³ç­– | é€‰æ‹©å·¥å…·: {action.tool} | è¾“å…¥: {action.tool_input[:200]}")

    def _start_timer(self, event_type):
        self.timers[event_type] = time.time()

    def _end_timer(self, event_type):
        return time.time() - self.timers.pop(event_type, time.time())

    def _get_timestamp(self):
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    def _format_inputs(self, inputs):
        return json.dumps(inputs, indent=2, ensure_ascii=False)[:300]

    def _format_outputs(self, outputs):
        return json.dumps(outputs, indent=2, ensure_ascii=False)[:300]

# 1. å®šä¹‰çŠ¶æ€æ¨¡å‹
class AgentState(TypedDict):
    """æ™ºèƒ½ä½“çŠ¶æ€æ¨¡å‹ï¼Œç”¨äºç»´æŠ¤ç³»ç»Ÿè¿è¡ŒçŠ¶æ€ã€‚"""
    messages: List[Any]  # å¯¹è¯å†å²
    task_status: str     # ä»»åŠ¡çŠ¶æ€
    task_plan: Dict      # ä»»åŠ¡è§„åˆ’
    execution_context: Dict  # æ‰§è¡Œä¸Šä¸‹æ–‡
    results: Dict        # æ‰§è¡Œç»“æœ
    memory: Dict         # è®°å¿†çŠ¶æ€
    context: Dict        # ä¸Šä¸‹æ–‡ä¿¡æ¯

def get_llm()->ChatOpenAI:
    llm = ChatOpenAI(
        model=os.getenv("MODEL", "deepseek-chat"),
        temperature=0.7,
        callbacks=[CustomCallbackHandler()]
    )
    return llm

# 2. å·¥å…·å®šä¹‰
@tools.tool
def browse_web(url: str) -> Dict:
    """æµè§ˆç½‘é¡µå¹¶æå–æ—…æ¸¸ç›¸å…³ä¿¡æ¯"""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
        paragraphs = soup.find_all('p')
        content = "\n".join([p.get_text().strip() for p in paragraphs[:15]])
        
        # æå–æ—…æ¸¸ç›¸å…³ä¿¡æ¯
        travel_keywords = ["æ—…æ¸¸", "æ™¯ç‚¹", "é…’åº—", "ç¾é£Ÿ", "äº¤é€š", "é—¨ç¥¨", "æ”»ç•¥", "è¡Œç¨‹", "ä½å®¿"]
        travel_paragraphs = []
        for p in paragraphs:
            text = p.get_text().strip()
            if any(keyword in text for keyword in travel_keywords):
                travel_paragraphs.append(text)
        
        # æå–é“¾æ¥
        links = []
        for link in soup.find_all('a', href=True)[:8]:
            link_text = link.get_text().strip() or "æ— æ–‡æœ¬"
            if any(keyword in link_text for keyword in travel_keywords):
                links.append({"text": link_text, "url": link['href']})
        
        return {
            "title": title,
            "content": content[:1500] + ("..." if len(content) > 1500 else ""),
            "travel_content": "\n".join(travel_paragraphs[:10]),
            "links": links,
            "status": "success"
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "url": url
        }

@tools.tool
def analyze_data(data: Dict, analysis_type: str = "basic") -> Dict:
    """åˆ†ææ•°æ®å·¥å…·ï¼šå¯¹ç½‘é¡µå†…å®¹è¿›è¡Œå¤šç»´åº¦åˆ†æ"""
    results = {}
    
    # æ•°æ®éªŒè¯
    if not isinstance(data, dict):
        return {"status": "error", "error": "è¾“å…¥æ•°æ®å¿…é¡»æ˜¯å­—å…¸æ ¼å¼"}

    if 'content' not in data:
        return {"status": "error", "error": "ç¼ºå°‘contentå­—æ®µ"}

    if not isinstance(data['content'], str):
        return {"status": "error", "error": "contentå­—æ®µå¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹"}

    if not data['content'].strip():
        return {"status": "error", "error": "contentå­—æ®µä¸èƒ½ä¸ºç©º"}

    # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
    soup = BeautifulSoup(data['content'], 'html.parser')
    
    # æå–çº¯æ–‡æœ¬å†…å®¹
    text_content = soup.get_text(separator='\n', strip=True)
    
    # åŸºç¡€åˆ†æ
    if analysis_type in ["basic", "all"]:
        paragraphs = [p for p in text_content.split('\n') if p.strip()]
        words = list(jieba.cut(text_content))
        results["basic"] = {
            "total_chars": len(text_content),
            "total_words": len(words),
            "paragraphs": len(paragraphs),
            "links": len(soup.find_all('a')),
            "images": len(soup.find_all('img'))
        }
    
    # ç»Ÿè®¡åˆ†æ
    if analysis_type in ["stats", "all"]:
        # è¯é¢‘ç»Ÿè®¡ï¼ˆå–å‰20ä¸ªæœ€å¸¸è§çš„è¯ï¼‰
        words = [w for w in jieba.cut(text_content) if len(w.strip()) > 1]
        word_freq = Counter(words).most_common(20)
        
        # å­—ç¬¦ç±»å‹åˆ†å¸ƒ
        char_types = {
            "chinese": len(re.findall(r'[\u4e00-\u9fff]', text_content)),
            "english": len(re.findall(r'[a-zA-Z]', text_content)),
            "digit": len(re.findall(r'\d', text_content)),
            "punctuation": len(re.findall(r'[\.,!?;:"]', text_content))
        }
        
        results["stats"] = {
            "word_frequency": dict(word_freq),
            "character_distribution": char_types
        }
    
    # æå–å…³é”®ä¿¡æ¯
    if analysis_type in ["extract", "all"]:
        # æå–æ ‡é¢˜
        titles = {
            "h1": [h.get_text(strip=True) for h in soup.find_all('h1')],
            "h2": [h.get_text(strip=True) for h in soup.find_all('h2')],
            "h3": [h.get_text(strip=True) for h in soup.find_all('h3')]
        }
        
        # æå–é“¾æ¥
        links = [{
            "text": a.get_text(strip=True),
            "href": a.get('href')
        } for a in soup.find_all('a') if a.get('href')]
        
        # æå–å›¾ç‰‡
        images = [{
            "alt": img.get('alt', ''),
            "src": img.get('src')
        } for img in soup.find_all('img') if img.get('src')]
        
        results["extracted"] = {
            "titles": titles,
            "links": links[:10],  # é™åˆ¶åªè¿”å›å‰10ä¸ªé“¾æ¥
            "images": images[:10]  # é™åˆ¶åªè¿”å›å‰10ä¸ªå›¾ç‰‡
        }
    
    return {"status": "success", "analysis_result": results}

@tools.tool
def search_knowledge_base(query: str) -> Dict:
    """æœç´¢çŸ¥è¯†åº“å·¥å…·ï¼Œç”¨äºæ£€ç´¢å·²å­˜å‚¨çš„çŸ¥è¯†å†…å®¹"""
    # æ¨¡æ‹ŸçŸ¥è¯†åº“æ£€ç´¢
    results = [
        {
            "content": f"å…³äº{query}çš„çŸ¥è¯†æ¡ç›®1",
            "source": "æ—…æ¸¸ç™¾ç§‘",
            "timestamp": datetime.now().isoformat()
        },
        {
            "content": f"å…³äº{query}çš„çŸ¥è¯†æ¡ç›®2",
            "source": "æ—…æ¸¸æ”»ç•¥",
            "timestamp": datetime.now().isoformat()
        }
    ]
    
    return {
        "query": query,
        "results": results,
        "total_results": len(results),
        "status": "success"
    }

@tools.tool
def summarize_text(text: str, max_length: int = 200) -> Dict:
    """å¯¹é•¿æ–‡æœ¬è¿›è¡Œæ‘˜è¦"""
    # åˆ†å‰²æ–‡æœ¬ä¸ºå¥å­
    sentences = [s.strip() for s in text.replace('\n', ' ').split('.') if s.strip()]
    
    if not sentences:
        return {"summary": "", "original_length": len(text), "summary_length": 0, "status": "error"}
    
    # è®¡ç®—æ¯ä¸ªå¥å­çš„é‡è¦æ€§ï¼ˆè¿™é‡Œç®€å•åœ°ä½¿ç”¨å¥å­é•¿åº¦ï¼‰
    importance = [len(s.split()) for s in sentences]
    
    # é€‰æ‹©æœ€é‡è¦çš„å¥å­ï¼ˆè¿™é‡Œç®€å•åœ°é€‰æ‹©æœ€é•¿çš„å¥å­ï¼‰
    sorted_sentences = [s for _, s in sorted(zip(importance, sentences), reverse=True)]
    
    # ç”Ÿæˆæ‘˜è¦
    summary = ". ".join(sorted_sentences[:3]) + "."
    
    # ç¡®ä¿æ‘˜è¦ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
    if len(summary) > max_length:
        summary = summary[:max_length-3] + "..."
    
    return {
        "summary": summary,
        "original_length": len(text),
        "summary_length": len(summary),
        "status": "success"
    }

@tools.tool
def get_weather(destination: str) -> Dict:
    """è·å–ç›®çš„åœ°çš„å¤©æ°”ä¿¡æ¯"""
    # è¿™é‡Œå¯ä»¥è°ƒç”¨å¤©æ°”APIè·å–å¤©æ°”ä¿¡æ¯
    # æ¨¡æ‹Ÿå¤©æ°”æ•°æ®
    weather_data = {
        "åŒ—äº¬": {"condition": "æ™´å¤©", "temperature": "25Â°C", "humidity": "40%"},
        "ä¸Šæµ·": {"condition": "å¤šäº‘", "temperature": "28Â°C", "humidity": "65%"},
        "å¹¿å·": {"condition": "å°é›¨", "temperature": "30Â°C", "humidity": "80%"},
        "æ·±åœ³": {"condition": "é˜µé›¨", "temperature": "29Â°C", "humidity": "75%"},
        "æˆéƒ½": {"condition": "é˜´å¤©", "temperature": "22Â°C", "humidity": "60%"},
        "æ­å·": {"condition": "æ™´å¤©", "temperature": "26Â°C", "humidity": "55%"},
        "è¥¿å®‰": {"condition": "æ™´å¤©", "temperature": "24Â°C", "humidity": "45%"},
        "é‡åº†": {"condition": "å¤šäº‘", "temperature": "27Â°C", "humidity": "70%"},
        "å¦é—¨": {"condition": "æ™´å¤©", "temperature": "29Â°C", "humidity": "60%"},
        "ä¸‰äºš": {"condition": "æ™´å¤©", "temperature": "32Â°C", "humidity": "75%"}
    }
    
    # è·å–å¤©æ°”æ•°æ®ï¼Œå¦‚æœç›®çš„åœ°ä¸åœ¨é¢„è®¾æ•°æ®ä¸­ï¼Œè¿”å›é»˜è®¤æ•°æ®
    weather = weather_data.get(destination, {"condition": "æ™´å¤©", "temperature": "25Â°C", "humidity": "50%"})
    
    return {
        "destination": destination,
        "weather": weather["condition"],
        "temperature": weather["temperature"],
        "humidity": weather["humidity"],
        "forecast": [
            {"day": "ä»Šå¤©", "condition": weather["condition"], "temperature": weather["temperature"]},
            {"day": "æ˜å¤©", "condition": "å¤šäº‘", "temperature": "26Â°C"},
            {"day": "åå¤©", "condition": "æ™´å¤©", "temperature": "27Â°C"}
        ],
        "status": "success"
    }

@tools.tool
def get_travel_recommendations(destination: str, interests: List[str], budget: str, duration: int) -> Dict:
    """è·å–æ—…è¡Œæ¨èä¿¡æ¯ï¼ŒåŒ…æ‹¬æ™¯ç‚¹ã€ä½å®¿å’Œæ´»åŠ¨"""
    # è°ƒç”¨å¤§æ¨¡å‹è·å–æ—…è¡Œæ¨è
    llm = get_llm()
    
    prompt = f"""
    è¯·ä¸ºç›®çš„åœ°"{destination}"ç”Ÿæˆæ—…è¡Œæ¨èï¼ŒåŒ…æ‹¬æ™¯ç‚¹ã€ä½å®¿ã€æ´»åŠ¨å’Œç¾é£Ÿã€‚
    ç”¨æˆ·å…´è¶£: {', '.join(interests)}
    é¢„ç®—çº§åˆ«: {budget}
    æ—…è¡Œå¤©æ•°: {duration}
    """
    
    response = llm.invoke([HumanMessage(content=prompt)])
    
    # å‡è®¾å¤§æ¨¡å‹è¿”å›çš„å†…å®¹æ˜¯ä¸€ä¸ªå­—å…¸æ ¼å¼
    return {
        "destination": destination,
        "recommendations": response.content,
        "status": "success"
    }

@tools.tool
def calculate_travel_budget(destination: str, duration: int, accommodation_level: str, activities: List[str]) -> Dict:
    """è®¡ç®—æ—…è¡Œé¢„ç®—"""
    # è°ƒç”¨å¤§æ¨¡å‹è®¡ç®—é¢„ç®—
    llm = get_llm()
    
    prompt = f"""
    è¯·ä¸ºç›®çš„åœ°"{destination}"è®¡ç®—æ—…è¡Œé¢„ç®—ã€‚
    æ—…è¡Œå¤©æ•°: {duration}
    ä½å®¿çº§åˆ«: {accommodation_level}
    æ´»åŠ¨: {', '.join(activities)}
    """
    
    response = llm.invoke([HumanMessage(content=prompt)])
    
    # å‡è®¾å¤§æ¨¡å‹è¿”å›çš„å†…å®¹æ˜¯ä¸€ä¸ªå­—å…¸æ ¼å¼
    return {
        "destination": destination,
        "duration": duration,
        "accommodation_level": accommodation_level,
        "budget_details": response.content,
        "status": "success"
    }

# 3. è®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†
class MemoryManager:
    """è®°å¿†ç®¡ç†å™¨ï¼šç®¡ç†çŸ­æœŸå’Œé•¿æœŸè®°å¿†"""
    
    def __init__(self):
        self.short_term_memory = []
        self.long_term_memory = []
        self.importance_threshold = 0.7
    
    def add_to_short_term(self, memory_item: Dict) -> None:
        """æ·»åŠ åˆ°çŸ­æœŸè®°å¿†"""
        if not isinstance(memory_item, dict):
            memory_item = {"content": str(memory_item)}
        
        if "timestamp" not in memory_item:
            memory_item["timestamp"] = datetime.now().isoformat()
        
        self.short_term_memory.append(memory_item)
        
        # å¦‚æœçŸ­æœŸè®°å¿†è¶…è¿‡å®¹é‡ï¼Œç§»é™¤æœ€æ—§çš„è®°å¿†
        if len(self.short_term_memory) > 50:
            self.short_term_memory.pop(0)
    
    def promote_to_long_term(self, index: int, importance: float) -> bool:
        """å°†çŸ­æœŸè®°å¿†æå‡ä¸ºé•¿æœŸè®°å¿†"""
        if importance < self.importance_threshold:
            return False
        
        if 0 <= index < len(self.short_term_memory):
            memory_item = self.short_term_memory[index].copy()
            memory_item["importance"] = importance
            self.long_term_memory.append(memory_item)
            return True
        
        return False
    
    def retrieve_relevant(self, query: str, limit: int = 5) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³è®°å¿†"""
        # ç®€å•å®ç°ï¼šåŸºäºå…³é”®è¯åŒ¹é…
        relevant_memories = []
        
        # æœç´¢é•¿æœŸè®°å¿†
        for memory in self.long_term_memory:
            content = memory.get("content", "")
            if isinstance(content, str) and any(keyword in content for keyword in query.split()):
                relevant_memories.append(memory)
        
        # æœç´¢çŸ­æœŸè®°å¿†
        for memory in reversed(self.short_term_memory):
            content = memory.get("content", "")
            if isinstance(content, str) and any(keyword in content for keyword in query.split()):
                relevant_memories.append(memory)
                
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_memories = []
        memory_contents = set()
        
        for memory in relevant_memories:
            content = str(memory.get("content", ""))
            if content not in memory_contents:
                memory_contents.add(content)
                unique_memories.append(memory)
                if len(unique_memories) >= limit:
                    break
        
        return unique_memories
    
    def get_memory_summary(self) -> Dict:
        """è·å–è®°å¿†çŠ¶æ€æ‘˜è¦"""
        return {
            "short_term_count": len(self.short_term_memory),
            "long_term_count": len(self.long_term_memory),
            "latest_short_term": self.short_term_memory[-1] if self.short_term_memory else None,
            "latest_long_term": self.long_term_memory[-1] if self.long_term_memory else None
        }

class ContextManager:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šç®¡ç†ä»»åŠ¡æ‰§è¡Œä¸Šä¸‹æ–‡"""
    
    def __init__(self):
        self.context = {}
        self.context_history = []
    
    def update_context(self, key: str, value: Any) -> None:
        """æ›´æ–°ä¸Šä¸‹æ–‡"""
        self.context[key] = value
    
    def get_context(self, key: str, default: Any = None) -> Any:
        """è·å–ä¸Šä¸‹æ–‡å€¼"""
        return self.context.get(key, default)
    
    def save_context_snapshot(self) -> None:
        """ä¿å­˜ä¸Šä¸‹æ–‡å¿«ç…§"""
        snapshot = {
            "timestamp": datetime.now().isoformat(),
            "context": self.context.copy()
        }
        self.context_history.append(snapshot)
        
        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(self.context_history) > 20:
            self.context_history.pop(0)
    
    def get_context_history(self) -> List[Dict]:
        """è·å–ä¸Šä¸‹æ–‡å†å²"""
        return self.context_history

# 4. ä»»åŠ¡åˆ†è§£ä¸è§„åˆ’
def decompose_task(task_description: str) -> List[Dict]:
    """å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡"""
    # æ ¹æ®ä»»åŠ¡æè¿°åŠ¨æ€ç”Ÿæˆå­ä»»åŠ¡
    if any(keyword in task_description for keyword in ["æ—…è¡Œ", "æ—…æ¸¸", "è¡Œç¨‹", "æ—…è¡Œè®¡åˆ’", "æ—…æ¸¸è®¡åˆ’", "å‡ºæ¸¸", "åº¦å‡"]):
        return [
            {"id": 1, "name": "ç¡®å®šæ—…è¡Œç›®çš„åœ°", "tools": ["browse_web"], "description": "æ ¹æ®ç”¨æˆ·éœ€æ±‚ç¡®å®šæœ€ä½³æ—…è¡Œç›®çš„åœ°"},
            {"id": 2, "name": "æ”¶é›†ç›®çš„åœ°ä¿¡æ¯", "tools": ["browse_web", "search_knowledge_base"], "description": "æ”¶é›†ç›®çš„åœ°çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ™¯ç‚¹ã€æ–‡åŒ–ã€ç¾é£Ÿç­‰"},
            {"id": 3, "name": "åˆ¶å®šè¡Œç¨‹å®‰æ’", "tools": ["analyze_data"], "description": "æ ¹æ®ç›®çš„åœ°ä¿¡æ¯åˆ¶å®šè¯¦ç»†çš„æ—¥ç¨‹å®‰æ’"},
            {"id": 4, "name": "æŸ¥æ‰¾ä½å®¿é€‰é¡¹", "tools": ["browse_web"], "description": "æŸ¥æ‰¾å¹¶æ¨èåˆé€‚çš„é…’åº—æˆ–ä½å®¿é€‰æ‹©"},
            {"id": 5, "name": "è§„åˆ’äº¤é€šæ–¹æ¡ˆ", "tools": ["browse_web"], "description": "è§„åˆ’å¾€è¿”äº¤é€šå’Œå½“åœ°äº¤é€šæ–¹æ¡ˆ"},
            {"id": 6, "name": "æ¨èç‰¹è‰²ä½“éªŒ", "tools": ["browse_web", "search_knowledge_base"], "description": "æ¨èç›®çš„åœ°çš„ç‰¹è‰²ä½“éªŒå’Œæ´»åŠ¨"},
            {"id": 7, "name": "è·å–å¤©æ°”ä¿¡æ¯", "tools": ["get_weather"], "description": "è·å–ç›®çš„åœ°çš„å¤©æ°”ä¿¡æ¯å¹¶æä¾›ç©¿ç€å»ºè®®"},
            {"id": 8, "name": "é¢„ç®—è§„åˆ’", "tools": ["analyze_data"], "description": "ä¼°ç®—æ—…è¡Œé¢„ç®—å¹¶æä¾›çœé’±å»ºè®®"},
            {"id": 9, "name": "ç”Ÿæˆå®Œæ•´æ—…è¡Œè®¡åˆ’", "tools": ["summarize_text"], "description": "æ•´åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆå®Œæ•´çš„æ—…è¡Œè®¡åˆ’"}
        ]
    else:
        # é»˜è®¤ä»»åŠ¡åˆ†è§£
        return [
            {"id": 1, "name": "ä¿¡æ¯æ”¶é›†", "tools": ["browse_web"], "description": "æ”¶é›†ç›¸å…³ä¿¡æ¯"},
            {"id": 2, "name": "æ•°æ®åˆ†æ", "tools": ["analyze_data"], "description": "åˆ†ææ”¶é›†çš„æ•°æ®"},
            {"id": 3, "name": "ç”ŸæˆæŠ¥å‘Š", "tools": ["summarize_text"], "description": "ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"}
        ]

def plan_execution(subtasks: List[Dict]) -> Dict:
    """è§„åˆ’ä»»åŠ¡æ‰§è¡Œé¡ºåº"""
    return {
        "execution_plan": subtasks,
        "total_steps": len(subtasks),
        "estimated_time": len(subtasks) * 5,  # æ¯ä¸ªå­ä»»åŠ¡ä¼°è®¡5åˆ†é’Ÿ
        "created_at": datetime.now().isoformat()
    }

# 5. å·¥ä½œæµèŠ‚ç‚¹å‡½æ•°
def understand_request(state: AgentState) -> AgentState:
    """ç†è§£ç”¨æˆ·è¯·æ±‚å¹¶è§„åˆ’ä»»åŠ¡"""
    llm = get_llm()
    messages = state["messages"]
    
    # åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
    memory_manager = state.get("memory", {}).get("manager", MemoryManager())
    context_manager = state.get("context", {}).get("manager", ContextManager())
    
    # è·å–ç›¸å…³è®°å¿†
    relevant_memories = memory_manager.retrieve_relevant(messages[-1].content)
    
    # æ„å»ºæç¤ºï¼ŒåŒ…å«ç›¸å…³è®°å¿†
    memory_context = "\n".join([f"- {m.get('content', '')}" for m in relevant_memories])
    
    # åˆ†æç”¨æˆ·è¯·æ±‚
    analysis_prompt = f"""
    è¯¦ç»†åˆ†æä»¥ä¸‹æ—…è¡Œè®¡åˆ’éœ€æ±‚ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶æ¨æ–­ç”¨æˆ·å¯èƒ½éœ€è¦çš„ä¿¡æ¯ã€‚
    
    ç”¨æˆ·æ¶ˆæ¯: {messages[-1].content}
    
    ç›¸å…³ä¸Šä¸‹æ–‡:
    {memory_context}
    
    è¯·æä¾›ä»¥ä¸‹ä¿¡æ¯:
    1. æ—…è¡Œä¸»è¦ç›®æ ‡å’Œç±»å‹ï¼ˆå¦‚ä¼‘é—²ã€æ¢é™©ã€æ–‡åŒ–ä½“éªŒç­‰ï¼‰
    2. å¯èƒ½çš„ç›®çš„åœ°åå¥½
    3. æ—…è¡Œæ—¶é—´å’ŒæŒç»­æ—¶é—´ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œè¯·æ¨æ–­åˆç†çš„æ—¶é—´ï¼‰
    4. æ—…è¡Œé¢„ç®—çº§åˆ«ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œè¯·æ¨æ–­åˆç†çš„é¢„ç®—ï¼‰
    5. ç‰¹æ®Šå…´è¶£æˆ–åå¥½ï¼ˆå¦‚ç¾é£Ÿã€å†å²ã€è‡ªç„¶é£å…‰ç­‰ï¼‰
    6. æ—…è¡ŒåŒä¼´æƒ…å†µï¼ˆå¦‚ç‹¬è‡ªã€å®¶åº­ã€æƒ…ä¾£ç­‰ï¼‰
    7. å…¶ä»–å¯èƒ½çš„é™åˆ¶æ¡ä»¶æˆ–ç‰¹æ®Šéœ€æ±‚
    
    å¯¹äºç”¨æˆ·æœªæ˜ç¡®æŒ‡å®šçš„ä¿¡æ¯ï¼Œè¯·åŸºäºä¸Šä¸‹æ–‡å’Œå¸¸è¯†è¿›è¡Œåˆç†æ¨æ–­ï¼Œä»¥ä¾¿èƒ½å¤Ÿåˆ¶å®šå®Œæ•´çš„æ—…è¡Œè®¡åˆ’ã€‚
    """
    
    response = llm.invoke([HumanMessage(content=analysis_prompt)])
    
    # ç›´æ¥è°ƒç”¨decompose_taskå‡½æ•°è¿›è¡Œä»»åŠ¡åˆ†è§£
    subtasks = decompose_task(messages[-1].content)
    
    # è§„åˆ’æ‰§è¡Œé¡ºåº
    execution_plan = plan_execution(subtasks)
    
    # æ›´æ–°ä¸Šä¸‹æ–‡
    context_manager.update_context("task_analysis", response.content)
    context_manager.update_context("current_task", messages[-1].content)
    
    # æå–ç›®çš„åœ°ä¿¡æ¯
    destination_prompt = f"""
    åŸºäºä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯å’Œåˆ†æï¼Œç¡®å®šæœ€åˆé€‚çš„æ—…è¡Œç›®çš„åœ°ã€‚
    
    ç”¨æˆ·æ¶ˆæ¯: {messages[-1].content}
    
    åˆ†æç»“æœ: {response.content}
    
    å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†ç›®çš„åœ°ï¼Œè¯·ç›´æ¥æå–ã€‚
    å¦‚æœç”¨æˆ·æœªæŒ‡å®šç›®çš„åœ°ä½†æœ‰åå¥½ï¼Œè¯·æ¨èæœ€åˆé€‚çš„ç›®çš„åœ°ã€‚
    å¦‚æœç”¨æˆ·å®Œå…¨æœªæåŠç›®çš„åœ°ï¼Œè¯·æ ¹æ®åˆ†æç»“æœæ¨èä¸€ä¸ªçƒ­é—¨ä¸”é€‚åˆçš„æ—…æ¸¸ç›®çš„åœ°ã€‚
    
    åªè¿”å›ç›®çš„åœ°åç§°ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
    """
    
    destination_response = llm.invoke([HumanMessage(content=destination_prompt)])
    destination = destination_response.content.strip()
    context_manager.update_context("destination", destination)
    
    # æ¨æ–­æ—…è¡Œæ—¶é—´å’ŒæŒç»­æ—¶é—´
    duration_prompt = f"""
    åŸºäºä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯å’Œåˆ†æï¼Œç¡®å®šæœ€åˆé€‚çš„æ—…è¡ŒæŒç»­æ—¶é—´ã€‚
    
    ç”¨æˆ·æ¶ˆæ¯: {messages[-1].content}
    
    åˆ†æç»“æœ: {response.content}
    
    å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†æ—…è¡Œæ—¶é—´å’ŒæŒç»­æ—¶é—´ï¼Œè¯·ç›´æ¥æå–ã€‚
    å¦‚æœç”¨æˆ·æœªæŒ‡å®šï¼Œè¯·æ ¹æ®ç›®çš„åœ°"{destination}"å’Œæ—…è¡Œç±»å‹æ¨èåˆé€‚çš„æŒç»­æ—¶é—´ï¼ˆå¤©æ•°ï¼‰ã€‚
    
    åªè¿”å›å¤©æ•°ï¼Œä¾‹å¦‚"5"è¡¨ç¤º5å¤©4æ™šï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
    """
    
    duration_response = llm.invoke([HumanMessage(content=duration_prompt)])
    try:
        duration = int(duration_response.content.strip())
    except:
        duration = 5  # é»˜è®¤5å¤©
    context_manager.update_context("duration", duration)
    
    # æ¨æ–­æ—…è¡Œé¢„ç®—
    budget_prompt = f"""
    åŸºäºä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯å’Œåˆ†æï¼Œç¡®å®šæ—…è¡Œé¢„ç®—çº§åˆ«ã€‚
    
    ç”¨æˆ·æ¶ˆæ¯: {messages[-1].content}
    
    åˆ†æç»“æœ: {response.content}
    
    å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†é¢„ç®—ï¼Œè¯·ç›´æ¥æå–ã€‚
    å¦‚æœç”¨æˆ·æœªæŒ‡å®šï¼Œè¯·æ ¹æ®ç›®çš„åœ°"{destination}"å’Œæ—…è¡ŒæŒç»­æ—¶é—´{duration}å¤©æ¨èåˆé€‚çš„é¢„ç®—çº§åˆ«ã€‚
    
    è¯·ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š
    - ç»æµå‹
    - èˆ’é€‚å‹
    - è±ªåå‹
    
    åªè¿”å›é¢„ç®—çº§åˆ«ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
    """
    
    budget_response = llm.invoke([HumanMessage(content=budget_prompt)])
    budget = budget_response.content.strip()
    context_manager.update_context("budget", budget)
    
    # æ¨æ–­æ—…è¡Œç±»å‹å’Œç‰¹æ®Šå…´è¶£
    interests_prompt = f"""
    åŸºäºä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯å’Œåˆ†æï¼Œç¡®å®šç”¨æˆ·çš„æ—…è¡Œç±»å‹å’Œç‰¹æ®Šå…´è¶£ã€‚
    
    ç”¨æˆ·æ¶ˆæ¯: {messages[-1].content}
    
    åˆ†æç»“æœ: {response.content}
    
    è¯·ä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©æœ€å¤š3ä¸ªä¸ç”¨æˆ·å…´è¶£ç›¸ç¬¦çš„é€‰é¡¹ï¼š
    - è‡ªç„¶é£å…‰
    - å†å²æ–‡åŒ–
    - ç¾é£Ÿä½“éªŒ
    - è´­ç‰©å¨±ä¹
    - å†’é™©æ´»åŠ¨
    - ä¼‘é—²æ”¾æ¾
    - è‰ºæœ¯æ¬£èµ
    - ä½“è‚²è¿åŠ¨
    
    ä»¥JSONæ ¼å¼è¿”å›ï¼Œä¾‹å¦‚ï¼š["è‡ªç„¶é£å…‰", "å†å²æ–‡åŒ–", "ç¾é£Ÿä½“éªŒ"]
    """
    
    interests_response = llm.invoke([HumanMessage(content=interests_prompt)])
    try:
        interests = json.loads(interests_response.content.strip())
    except:
        interests = ["è‡ªç„¶é£å…‰", "å†å²æ–‡åŒ–", "ç¾é£Ÿä½“éªŒ"]
    context_manager.update_context("interests", interests)
    
    # æ¨æ–­æ—…è¡ŒåŒä¼´æƒ…å†µ
    companions_prompt = f"""
    åŸºäºä»¥ä¸‹ç”¨æˆ·æ¶ˆæ¯å’Œåˆ†æï¼Œç¡®å®šç”¨æˆ·çš„æ—…è¡ŒåŒä¼´æƒ…å†µã€‚
    
    ç”¨æˆ·æ¶ˆæ¯: {messages[-1].content}
    
    åˆ†æç»“æœ: {response.content}
    
    è¯·ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š
    - ç‹¬è‡ªæ—…è¡Œ
    - æƒ…ä¾£å‡ºæ¸¸
    - å®¶åº­æ—…è¡Œ
    - æœ‹å‹ç»“ä¼´
    - å›¢é˜Ÿæ—…è¡Œ
    
    åªè¿”å›æ—…è¡ŒåŒä¼´ç±»å‹ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
    """
    
    companions_response = llm.invoke([HumanMessage(content=companions_prompt)])
    companions = companions_response.content.strip()
    context_manager.update_context("companions", companions)
    
    # ä¿å­˜ä¸Šä¸‹æ–‡å¿«ç…§
    context_manager.save_context_snapshot()
    
    # æ·»åŠ åˆ°è®°å¿†
    memory_manager.add_to_short_term({
        "type": "task",
        "content": messages[-1].content,
        "analysis": response.content,
        "inferred_details": {
            "destination": destination,
            "duration": duration,
            "budget": budget,
            "interests": interests,
            "companions": companions
        }
    })
    
    return {
        **state,
        "task_plan": execution_plan,
        "task_status": "planned",
        "execution_context": {"current_subtask": 0},
        "memory": {"manager": memory_manager},
        "context": {"manager": context_manager}
    }

def execute_task(state: AgentState) -> AgentState:
    """æ‰§è¡Œä»»åŠ¡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    context = state["execution_context"]
    plan = state["task_plan"]
    current_subtask = plan["execution_plan"][context["current_subtask"]]
    
    # è·å–è®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
    memory_manager = state.get("memory", {}).get("manager", MemoryManager())
    context_manager = state.get("context", {}).get("manager", ContextManager())

    tools_map = {
        "browse_web": browse_web,
        "analyze_data": analyze_data,
        "search_knowledge_base": search_knowledge_base,
        "summarize_text": summarize_text,
        "get_weather": get_weather,
        "get_travel_recommendations": get_travel_recommendations,
        "calculate_travel_budget": calculate_travel_budget
    }
    
    # åˆå§‹åŒ–æ‰§è¡Œç»“æœå’Œæ—¥å¿—
    results = {}
    execution_log = context.get("logs", [])
    
    # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
    destination = context_manager.get_context("destination", "çƒ­é—¨æ—…æ¸¸ç›®çš„åœ°")
    duration = context_manager.get_context("duration", 5)
    budget = context_manager.get_context("budget", "èˆ’é€‚å‹")
    interests = context_manager.get_context("interests", ["è‡ªç„¶é£å…‰", "å†å²æ–‡åŒ–", "ç¾é£Ÿä½“éªŒ"])
    companions = context_manager.get_context("companions", "å®¶åº­æ—…è¡Œ")
    
    # æ‰§è¡Œå½“å‰å­ä»»åŠ¡
    for tool_name in current_subtask["tools"]:
        # è·å–å·¥å…·å®ä¾‹
        tool = tools_map.get(tool_name)
        if not tool:
            error_info = f"å·¥å…· {tool_name} æœªæ‰¾åˆ°"
            memory_manager.add_to_short_term({
                "type": "error",
                "content": error_info,
                "subtask": current_subtask["name"]
            })
            return {
                **state,
                "task_status": "error",
                "error_info": error_info,
                "memory": {"manager": memory_manager},
                "context": {"manager": context_manager}
            }
        
        # å‡†å¤‡å·¥å…·è¾“å…¥å‚æ•°
        tool_input = {}
        
        if tool_name == "browse_web":
            # æ ¹æ®å­ä»»åŠ¡åç§°ç¡®å®šæœç´¢å†…å®¹
            if current_subtask["name"] == "ç¡®å®šæ—…è¡Œç›®çš„åœ°":
                tool_input = {"url": f"https://example.com/search?q=çƒ­é—¨æ—…æ¸¸ç›®çš„åœ°æ¨è"}
            elif current_subtask["name"] == "æ”¶é›†ç›®çš„åœ°ä¿¡æ¯":
                tool_input = {"url": f"https://example.com/search?q={destination}+æ—…æ¸¸æ”»ç•¥"}
            elif current_subtask["name"] == "æŸ¥æ‰¾ä½å®¿é€‰é¡¹":
                tool_input = {"url": f"https://example.com/search?q={destination}+{budget}+é…’åº—æ¨è"}
            elif current_subtask["name"] == "è§„åˆ’äº¤é€šæ–¹æ¡ˆ":
                tool_input = {"url": f"https://example.com/search?q=å‰å¾€{destination}+äº¤é€šæ–¹å¼"}
            elif current_subtask["name"] == "æ¨èç‰¹è‰²ä½“éªŒ":
                interests_str = "ã€".join(interests)
                tool_input = {"url": f"https://example.com/search?q={destination}+{interests_str}+ç‰¹è‰²ä½“éªŒ"}
            else:
                tool_input = {"url": f"https://example.com/search?q={destination}+æ—…æ¸¸"}
        elif tool_name == "analyze_data":
            # æ ¹æ®å­ä»»åŠ¡åç§°ç¡®å®šåˆ†æç±»å‹
            if current_subtask["name"] == "åˆ¶å®šè¡Œç¨‹å®‰æ’":
                tool_input = {
                    "data": state.get("results", {}).get("browse_web", {"content": f"{destination}æ—…æ¸¸ä¿¡æ¯"}),
                    "analysis_type": "extract"
                }
            elif current_subtask["name"] == "é¢„ç®—è§„åˆ’":
                tool_input = {
                    "data": {
                        "content": f"{destination} {duration}å¤© {budget}é¢„ç®— {companions}"
                    },
                    "analysis_type": "basic"
                }
            else:
                tool_input = {
                    "data": state.get("results", {}).get("browse_web", {"content": "æ—…æ¸¸ä¿¡æ¯"}),
                    "analysis_type": "basic"
                }
        elif tool_name == "search_knowledge_base":
            interests_str = "ã€".join(interests)
            tool_input = {"query": f"{destination} {interests_str} æ—…æ¸¸æ”»ç•¥"}
        elif tool_name == "summarize_text":
            # æ”¶é›†æ‰€æœ‰å·²è·å–çš„ä¿¡æ¯
            all_results = state.get("results", {})
            combined_text = ""
            for tool_result in all_results.values():
                if isinstance(tool_result, dict) and "content" in tool_result:
                    combined_text += tool_result["content"] + "\n\n"
                elif isinstance(tool_result, str):
                    combined_text += tool_result + "\n\n"
            
            tool_input = {"text": combined_text, "max_length": 1000}
        elif tool_name == "get_weather":
            tool_input = {"destination": destination}
        elif tool_name == "get_travel_recommendations":
            tool_input = {
                "destination": destination,
                "interests": interests,
                "budget": budget,
                "duration": duration
            }
        elif tool_name == "calculate_travel_budget":
            # è·å–æ´»åŠ¨åˆ—è¡¨
            activities = []
            if "get_travel_recommendations" in state.get("results", {}):
                rec_result = state["results"]["get_travel_recommendations"]
                if isinstance(rec_result, dict) and "recommended_spots" in rec_result:
                    activities = rec_result["recommended_spots"]
            
            tool_input = {
                "destination": destination,
                "duration": duration,
                "accommodation_level": budget,
                "activities": activities
            }
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        try:
            # è®°å½•å·¥å…·è°ƒç”¨å¼€å§‹
            execution_log.append({
                "timestamp": datetime.now().isoformat(),
                "tool": tool_name,
                "status": "started",
                "input": tool_input
            })
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            result = tool.invoke(tool_input)
            
            # è®°å½•æˆåŠŸæ—¥å¿—
            execution_log.append({
                "timestamp": datetime.now().isoformat(),
                "tool": tool_name,
                "status": "success",
                "output": result
            })
            
            # ä¿å­˜ç»“æœ
            results[tool_name] = result
            
            # æ·»åŠ åˆ°è®°å¿†
            memory_manager.add_to_short_term({
                "type": "tool_result",
                "tool": tool_name,
                "input": tool_input,
                "output": result,
                "subtask": current_subtask["name"]
            })
            
            # å¦‚æœç»“æœç‰¹åˆ«é‡è¦ï¼Œæå‡åˆ°é•¿æœŸè®°å¿†
            if tool_name in ["analyze_data", "search_knowledge_base", "get_travel_recommendations", "calculate_travel_budget"]:
                memory_manager.promote_to_long_term(len(memory_manager.short_term_memory) - 1, 0.8)
            
            # å¦‚æœæ˜¯ç¡®å®šç›®çš„åœ°ä»»åŠ¡ï¼Œæ›´æ–°ä¸Šä¸‹æ–‡ä¸­çš„ç›®çš„åœ°
            if current_subtask["name"] == "ç¡®å®šæ—…è¡Œç›®çš„åœ°" and tool_name == "browse_web":
                # ä»ç»“æœä¸­æå–å¯èƒ½çš„ç›®çš„åœ°
                if isinstance(result, dict) and "content" in result:
                    content = result["content"]
                    # ç®€å•çš„ç›®çš„åœ°æå–é€»è¾‘
                    popular_destinations = ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æˆéƒ½", "æ­å·", "è¥¿å®‰", "é‡åº†", "å¦é—¨", "ä¸‰äºš"]
                    for dest in popular_destinations:
                        if dest in content:
                            context_manager.update_context("destination", dest)
                            break
        except Exception as e:
            # è®°å½•é”™è¯¯æ—¥å¿—
            error_info = f"å·¥å…· {tool_name} æ‰§è¡Œé”™è¯¯: {str(e)}"
            execution_log.append({
                "timestamp": datetime.now().isoformat(),
                "tool": tool_name,
                "status": "error",
                "error": error_info
            })
            
            # æ·»åŠ åˆ°è®°å¿†
            memory_manager.add_to_short_term({
                "type": "error",
                "content": error_info,
                "subtask": current_subtask["name"]
            })
            
            return {
                **state,
                "task_status": "error",
                "error_info": error_info,
                "execution_context": {**context, "logs": execution_log},
                "memory": {"manager": memory_manager},
                "context": {"manager": context_manager}
            }
    
    # æ›´æ–°æ‰§è¡Œä¸Šä¸‹æ–‡
    updated_context = {
        **context,
        "logs": execution_log,
        "last_executed": current_subtask["name"],
        "last_executed_at": datetime.now().isoformat()
    }
    
    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šå­ä»»åŠ¡
    if context["current_subtask"] < len(plan["execution_plan"]) - 1:
        # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå­ä»»åŠ¡
        updated_context["current_subtask"] = context["current_subtask"] + 1
        task_status = "executing"
    else:
        # æ‰€æœ‰å­ä»»åŠ¡å·²å®Œæˆ
        task_status = "completed"
    
    # ä¿å­˜ä¸Šä¸‹æ–‡å¿«ç…§
    context_manager.save_context_snapshot()
    
    return {
        **state,
        "results": {**state.get("results", {}), **results},
        "task_status": task_status,
        "execution_context": updated_context,
        "memory": {"manager": memory_manager},
        "context": {"manager": context_manager}
    }

def handle_errors(state: AgentState) -> AgentState:
    """å¤„ç†æ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯"""
    # è·å–è®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
    memory_manager = state.get("memory", {}).get("manager", MemoryManager())
    context_manager = state.get("context", {}).get("manager", ContextManager())
    
    # è·å–é”™è¯¯ä¿¡æ¯
    error_info = state.get("error_info", "æœªçŸ¥é”™è¯¯")
    
    # è®°å½•é”™è¯¯
    memory_manager.add_to_short_term({
        "type": "error_handling",
        "content": f"å¤„ç†é”™è¯¯: {error_info}",
        "timestamp": datetime.now().isoformat()
    })
    
    # å°è¯•æ¢å¤æ‰§è¡Œ
    execution_context = state["execution_context"]
    task_plan = state["task_plan"]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šå­ä»»åŠ¡å¯ä»¥æ‰§è¡Œ
    if execution_context.get("current_subtask", 0) < len(task_plan.get("execution_plan", [])):
        # å¯ä»¥ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªå­ä»»åŠ¡
        return {
            **state,
            "task_status": "executing",
            "memory": {"manager": memory_manager},
            "context": {"manager": context_manager}
        }
    else:
        # æ— æ³•ç»§ç»­æ‰§è¡Œï¼Œæ ‡è®°ä¸ºå¤±è´¥
        messages = state["messages"] + [
            AIMessage(content=f"å¾ˆæŠ±æ­‰ï¼Œåœ¨æ‰§è¡Œä»»åŠ¡æ—¶é‡åˆ°äº†é—®é¢˜: {error_info}ã€‚æ— æ³•å®Œæˆæ—…è¡Œè®¡åˆ’çš„åˆ¶å®šã€‚")
        ]
        
        return {
            **state,
            "messages": messages,
            "task_status": "failed",
            "memory": {"manager": memory_manager},
            "context": {"manager": context_manager}
        }

def generate_report(state: AgentState) -> AgentState:
    """ç”Ÿæˆå¢å¼ºç‰ˆä»»åŠ¡æŠ¥å‘Šï¼ŒåŒ…å«æ‰§è¡Œå†å²å’Œè®°å¿†ä¿¡æ¯"""
    llm = get_llm()
    
    # è·å–è®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
    memory_manager = state.get("memory", {}).get("manager", MemoryManager())
    context_manager = state.get("context", {}).get("manager", ContextManager())
    
    # è·å–æ‰§è¡Œç»“æœå’Œä¸Šä¸‹æ–‡
    results = state["results"]
    execution_context = state["execution_context"]
    task_plan = state["task_plan"]
    
    # è·å–æ—…è¡Œç›¸å…³ä¿¡æ¯
    destination = context_manager.get_context("destination", "æœªæŒ‡å®šç›®çš„åœ°")
    duration = context_manager.get_context("duration", 5)
    budget = context_manager.get_context("budget", "èˆ’é€‚å‹")
    interests = context_manager.get_context("interests", ["è‡ªç„¶é£å…‰", "å†å²æ–‡åŒ–", "ç¾é£Ÿä½“éªŒ"])
    companions = context_manager.get_context("companions", "å®¶åº­æ—…è¡Œ")
    
    # æ„å»ºæŠ¥å‘Šæç¤º
    report_prompt = f"""
    è¯·ä¸ºç›®çš„åœ°"{destination}"ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„{duration}å¤©æ—…è¡Œè®¡åˆ’æŠ¥å‘Šã€‚
    
    æ—…è¡ŒåŸºæœ¬ä¿¡æ¯ï¼š
    - ç›®çš„åœ°: {destination}
    - æ—…è¡Œå¤©æ•°: {duration}å¤©
    - é¢„ç®—çº§åˆ«: {budget}
    - æ—…è¡ŒåŒä¼´: {companions}
    - å…´è¶£åå¥½: {', '.join(interests)}
    
    åŸºäºä»¥ä¸‹æ‰§è¡Œä¿¡æ¯ï¼š
    
    1. æ‰§è¡Œç»“æœï¼š
    {json.dumps(results, indent=2, ensure_ascii=False)}
    
    è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æ—…è¡Œè®¡åˆ’ï¼š
    
    1. ç›®çš„åœ°æ¦‚è§ˆï¼šç®€è¦ä»‹ç»{destination}çš„ç‰¹è‰²å’Œäº®ç‚¹ï¼Œä¸ºä»€ä¹ˆå®ƒé€‚åˆ{companions}å’Œ{', '.join(interests)}çš„ä½“éªŒã€‚
    
    2. è¯¦ç»†è¡Œç¨‹å®‰æ’ï¼š
       - è¯·ä¸º{duration}å¤©çš„æ—…è¡Œæä¾›æ¯å¤©çš„è¯¦ç»†è¡Œç¨‹
       - æ¯å¤©åº”åŒ…æ‹¬ä¸Šåˆã€ä¸‹åˆå’Œæ™šä¸Šçš„æ´»åŠ¨å®‰æ’
       - è€ƒè™‘æ™¯ç‚¹ä¹‹é—´çš„è·ç¦»å’Œæ¸¸è§ˆæ—¶é—´
       - å®‰æ’é€‚å½“çš„ç”¨é¤å’Œä¼‘æ¯æ—¶é—´
       - æ ¹æ®{', '.join(interests)}çš„å…´è¶£åå¥½å®‰æ’ç›¸åº”æ´»åŠ¨
    
    3. ä½å®¿æ¨èï¼š
       - æä¾›3-5ä¸ªç¬¦åˆ{budget}é¢„ç®—çš„ä½å®¿é€‰æ‹©
       - åŒ…æ‹¬ä½ç½®ã€ä»·æ ¼èŒƒå›´å’Œç‰¹è‰²
       - è¯´æ˜ä¸ºä»€ä¹ˆè¿™äº›ä½å®¿é€‚åˆ{companions}
    
    4. äº¤é€šä¿¡æ¯ï¼š
       - å¾€è¿”{destination}çš„äº¤é€šå»ºè®®
       - å½“åœ°äº¤é€šæ–¹å¼å’Œä½¿ç”¨å»ºè®®
       - å¦‚ä½•åœ¨æ™¯ç‚¹ä¹‹é—´é«˜æ•ˆç§»åŠ¨
    
    5. å¿…æ¸¸æ™¯ç‚¹å’Œç‰¹è‰²ä½“éªŒï¼š
       - åˆ—å‡º5-8ä¸ªå¿…æ¸¸æ™¯ç‚¹ï¼Œå¹¶è¯´æ˜ç‰¹è‰²
       - æ¨è2-3ä¸ªç¬¦åˆ{', '.join(interests)}å…´è¶£çš„ç‰¹è‰²ä½“éªŒ
       - æä¾›æœ€ä½³æ¸¸è§ˆæ—¶é—´å’Œå°è´´å£«
    
    6. ç¾é£Ÿæ¨èï¼š
       - æ¨èå½“åœ°ç‰¹è‰²ç¾é£Ÿå’Œé¤å…
       - åŒ…æ‹¬ä¸åŒä»·ä½çš„é€‰æ‹©
       - ç‰¹åˆ«æ ‡æ³¨é€‚åˆ{companions}çš„é¤å…
    
    7. è´­ç‰©æŒ‡å—ï¼š
       - æ¨èç‰¹è‰²å•†å“å’Œè´­ç‰©åœºæ‰€
       - ä»·æ ¼å‚è€ƒå’Œç ä»·æŠ€å·§
    
    8. å¤©æ°”å’Œç©¿ç€å»ºè®®ï¼š
       - æ ¹æ®å½“åœ°å¤©æ°”æƒ…å†µæä¾›ç©¿ç€å»ºè®®
       - å­£èŠ‚æ€§æ³¨æ„äº‹é¡¹
    
    9. é¢„ç®—è§„åˆ’ï¼š
       - æä¾›è¯¦ç»†çš„é¢„ç®—æ˜ç»†ï¼ˆäº¤é€šã€ä½å®¿ã€é¤é¥®ã€é—¨ç¥¨ã€è´­ç‰©ç­‰ï¼‰
       - çœé’±æŠ€å·§å’Œæ³¨æ„äº‹é¡¹
    
    10. æ—…è¡Œè´´å£«ï¼š
       - å½“åœ°æ–‡åŒ–ã€ä¹ ä¿—å’Œæ³¨æ„äº‹é¡¹
       - å®‰å…¨æç¤º
       - ç´§æ€¥è”ç³»æ–¹å¼å’ŒåŒ»ç–—ä¿¡æ¯
    
    æŠ¥å‘Šåº”å½“ä¸“ä¸šã€å…¨é¢ä¸”æ˜“äºç†è§£ï¼Œé€‚åˆ{companions}å‚è€ƒä½¿ç”¨ã€‚è¯·ç¡®ä¿è¡Œç¨‹å®‰æ’åˆç†ï¼Œæ´»åŠ¨ä¸°å¯Œå¤šæ ·ï¼Œèƒ½å¤Ÿå……åˆ†ä½“éªŒ{destination}çš„ç‰¹è‰²ã€‚
    """
    
    # è°ƒç”¨LLMç”ŸæˆæŠ¥å‘Š
    response = llm.invoke([HumanMessage(content=report_prompt)])
    
    # å°†æŠ¥å‘Šæ·»åŠ åˆ°å¯¹è¯å†å²
    messages = state["messages"] + [
        AIMessage(content=f"ã€{destination} {duration}å¤©{budget}æ—…è¡Œè®¡åˆ’ã€‘\n\n{response.content}")
    ]
    
    # å°†æŠ¥å‘Šæ·»åŠ åˆ°è®°å¿†
    memory_manager.add_to_short_term({
        "type": "report",
        "content": response.content,
        "timestamp": datetime.now().isoformat()
    })
    
    # å¦‚æœæ˜¯é‡è¦æŠ¥å‘Šï¼Œæå‡åˆ°é•¿æœŸè®°å¿†
    memory_manager.promote_to_long_term(len(memory_manager.short_term_memory) - 1, 0.9)
    
    # æ›´æ–°ä¸Šä¸‹æ–‡
    context_manager.update_context("final_report", response.content)
    context_manager.update_context("task_completed_at", datetime.now().isoformat())
    context_manager.save_context_snapshot()
    
    return {
        **state,
        "messages": messages,
        "task_status": "completed",
        "memory": {"manager": memory_manager},
        "context": {"manager": context_manager}
    }

def create_agent() -> StateGraph:
    """åˆ›å»ºå¹¶é…ç½®å·¥ä½œæµå®ä¾‹"""
    # åˆå§‹åŒ–å·¥ä½œæµ
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("understand_request", understand_request)
    workflow.add_node("execute_task", execute_task)
    workflow.add_node("handle_errors", handle_errors)
    workflow.add_node("generate_report", generate_report)
    
    # é…ç½®èŠ‚ç‚¹è½¬æ¢å…³ç³»
    workflow.set_entry_point("understand_request")
    
    workflow.add_edge("understand_request", "execute_task")
    
    # é…ç½®execute_taskèŠ‚ç‚¹çš„æ¡ä»¶è½¬æ¢
    workflow.add_conditional_edges(
        "execute_task",
        lambda x: "generate_report" if x["task_status"] == "completed" else "handle_errors",
        {
            "generate_report": "generate_report",
            "handle_errors": "handle_errors"
        }
    )
    
    # ä»generate_reportåˆ°END
    workflow.add_edge("generate_report", END)
    
    # é…ç½®handle_errorsèŠ‚ç‚¹çš„æ¡ä»¶è½¬æ¢
    workflow.add_conditional_edges(
        "handle_errors",
        lambda x: END if x["task_status"] == "failed" else "execute_task",
        {
            END: END,
            "execute_task": "execute_task"
        }
    )
    
    return workflow

def save_mermaid_graph(mermaid_code: str, output_file: str):
    with open("graph.mmd", "w") as f:
        f.write(mermaid_code)
    # ä½¿ç”¨ Mermaid CLI ç”Ÿæˆå›¾å½¢
    # mmdc -i graph.mmd -o agent.graph.png
    
    # ä½¿ç”¨ Mermaid CLI ç”Ÿæˆå›¾å½¢
    # subprocess.run(["mmdc", "-i", "graph.mmd", "-o", output_file])

def main():
    """ä¸»å‡½æ•°ï¼šåˆå§‹åŒ–å¹¶æ‰§è¡Œå·¥ä½œæµï¼Œå±•ç¤ºæ‰§è¡Œç»“æœ"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description='æ™ºèƒ½æ—…è¡Œè®¡åˆ’åŠ©æ‰‹')
    parser.add_argument('--task', type=str, help='è¦æ‰§è¡Œçš„ä»»åŠ¡æè¿°', default='è¯·å¸®æˆ‘åˆ¶å®šä¸€ä¸ªæ—…è¡Œè®¡åˆ’')
    parser.add_argument('--full', action='store_true', help='æ˜¾ç¤ºå®Œæ•´æŠ¥å‘Š', default=True)
    args = parser.parse_args()

    # åˆå§‹åŒ–è®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
    memory_manager = MemoryManager()
    context_manager = ContextManager()
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "messages": [HumanMessage(content=args.task)],
        "task_status": "new",
        "task_plan": {},
        "execution_context": {},
        "results": {},
        "memory": {"manager": memory_manager},
        "context": {"manager": context_manager}
    }

    # åˆ›å»ºAgentå®ä¾‹
    agent = create_agent().compile()
    mermaid_code = agent.get_graph().draw_mermaid()
    save_mermaid_graph(mermaid_code, "output_graph.png")

    # ç„¶åä½¿ç”¨ IPython æ˜¾ç¤ºç”Ÿæˆçš„å›¾å½¢
    display(Image("output_graph.png"))

    # æ‰§è¡Œå·¥ä½œæµ
    print("\nğŸš€ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ...\n")
    result = agent.invoke(initial_state)

    # å±•ç¤ºæ‰§è¡Œç»“æœ
    print("\nğŸ“Š æ‰§è¡Œç»“æœæ±‡æ€»ï¼š")
    print("-" * 50)
    print(f"âœ… ä»»åŠ¡çŠ¶æ€ï¼š{result['task_status']}")
    
    if result.get('task_plan'):
        print("\nğŸ“‹ ä»»åŠ¡è§„åˆ’ï¼š")
        for task in result['task_plan'].get('execution_plan', []):
            print(f"  - {task['name']} (ID: {task['id']})")
    
    if result.get('results'):
        print("\nğŸ¯ æ‰§è¡Œç»“æœï¼š")
        for key, value in result['results'].items():
            print(f"  - {key}: {str(value)[:100]}...")
    
    if result.get('messages'):
        print("\nğŸ’¬ æœ€ç»ˆæŠ¥å‘Šï¼š")
        final_report = result['messages'][-1].content
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºå®Œæ•´æŠ¥å‘Š
        if args.full:
            print(final_report)
        else:
            # æå–å¹¶æ˜¾ç¤ºæŠ¥å‘Šçš„ä¸»è¦éƒ¨åˆ†
            sections = extract_report_sections(final_report)
            for section_title, section_content in sections.items():
                print(f"\n### {section_title} ###")
                # å¯¹äºæ¯ä¸ªéƒ¨åˆ†æ˜¾ç¤ºæ‘˜è¦å†…å®¹
                if len(section_content) > 300:
                    print(section_content[:300] + "...")
                else:
                    print(section_content)
            
            print("\n(ä½¿ç”¨ --full å‚æ•°æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š)")
    
    # æ˜¾ç¤ºè®°å¿†å’Œä¸Šä¸‹æ–‡æ‘˜è¦
    if result.get('memory', {}).get('manager'):
        memory_summary = result['memory']['manager'].get_memory_summary()
        print("\nğŸ§  è®°å¿†æ‘˜è¦ï¼š")
        print(f"  - çŸ­æœŸè®°å¿†é¡¹æ•°: {memory_summary['short_term_count']}")
        print(f"  - é•¿æœŸè®°å¿†é¡¹æ•°: {memory_summary['long_term_count']}")

def extract_report_sections(report_text):
    """ä»æŠ¥å‘Šæ–‡æœ¬ä¸­æå–å„ä¸ªéƒ¨åˆ†"""
    sections = {}
    
    # å¸¸è§çš„æŠ¥å‘Šéƒ¨åˆ†æ ‡é¢˜
    section_patterns = [
        "ç›®çš„åœ°æ¦‚è§ˆ", "è¡Œç¨‹å®‰æ’", "ä½å®¿æ¨è", "äº¤é€šä¿¡æ¯", 
        "å¿…æ¸¸æ™¯ç‚¹", "ç‰¹è‰²ä½“éªŒ", "ç¾é£Ÿæ¨è", "è´­ç‰©æŒ‡å—", 
        "å¤©æ°”å’Œç©¿ç€å»ºè®®", "é¢„ç®—è§„åˆ’", "æ—…è¡Œè´´å£«"
    ]
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„éƒ¨åˆ†å†…å®¹
    import re
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„éƒ¨åˆ†æ ‡é¢˜
    all_headers = re.findall(r'#+\s*(.*?)\s*\n', report_text)
    all_headers.extend(re.findall(r'\d+\.\s*(.*?)\s*\n', report_text))
    all_headers.extend(re.findall(r'\*\*(.*?)\*\*', report_text))
    
    # æ·»åŠ æ‰¾åˆ°çš„æ ‡é¢˜åˆ°æ¨¡å¼ä¸­
    for header in all_headers:
        for pattern in section_patterns:
            if pattern.lower() in header.lower() and pattern not in section_patterns:
                section_patterns.append(header)
    
    # æå–å„éƒ¨åˆ†å†…å®¹
    current_section = "æ¦‚è¿°"
    sections[current_section] = ""
    
    for line in report_text.split('\n'):
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„éƒ¨åˆ†æ ‡é¢˜
        is_new_section = False
        for pattern in section_patterns:
            if pattern.lower() in line.lower() and (
                line.startswith('#') or 
                re.match(r'\d+\.', line) or 
                line.startswith('**')
            ):
                current_section = pattern
                sections[current_section] = line + "\n"
                is_new_section = True
                break
        
        if not is_new_section and current_section in sections:
            sections[current_section] += line + "\n"
    
    return sections

if __name__ == "__main__":
    main()