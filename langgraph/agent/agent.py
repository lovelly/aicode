from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Dict, Any
from langchain_core.messages import HumanMessage, AIMessage
import langchain_core.tools as tools
from langchain_openai import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import os
from dotenv import load_dotenv
from IPython.display import Image, display
import time
from datetime import datetime
import json

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class CustomCallbackHandler(BaseCallbackHandler):
    """å¢å¼ºç‰ˆå›è°ƒå¤„ç†å™¨ï¼Œæä¾›è¯¦ç»†æ‰§è¡Œç›‘æ§å’Œé”™è¯¯å¤„ç†"""
    
    def __init__(self):
        self.execution_stack = []
        self.timers = {}

    def on_llm_start(self, serialized, prompts, **kwargs):
        self._start_timer('llm')
        print(f"ğŸ•’ [{self._get_timestamp()}] LLMå¼€å§‹ç”Ÿæˆ | æç¤ºæ•°é‡: {len(prompts)}")

    def on_llm_end(self, response, **kwargs):
        duration = self._end_timer('llm')
        print(f"âœ… [{self._get_timestamp()}] LLMç”Ÿæˆå®Œæˆ | è€—æ—¶: {duration:.2f}s | å“åº”é•¿åº¦: {len(str(response))}")

    def on_tool_start(self, serialized, input_str, **kwargs):
        self._start_timer('tool')
        print(f"ğŸ”§ [{self._get_timestamp()}] å·¥å…·å¼€å§‹æ‰§è¡Œ | å·¥å…·: {serialized.get('name')} | è¾“å…¥: {input_str[:200]}")

    def on_tool_end(self, output, **kwargs):
        duration = self._end_timer('tool')
        print(f"âœ… [{self._get_timestamp()}] å·¥å…·æ‰§è¡Œå®Œæˆ | è€—æ—¶: {duration:.2f}s | è¾“å‡º: {str(output)[:200]}")

    def on_tool_error(self, error, **kwargs):
        print(f"âŒ [{self._get_timestamp()}] å·¥å…·æ‰§è¡Œé”™è¯¯ | é”™è¯¯ç±»å‹: {type(error).__name__} | è¯¦æƒ…: {str(error)[:500]}")

    def on_chain_start(self, serialized, inputs, **kwargs):
        self._start_timer('chain')
        chain_type = serialized.get('name', 'æœªçŸ¥é“¾')
        print(f"â›“ï¸ [{self._get_timestamp()}] é“¾å¼€å§‹æ‰§è¡Œ | ç±»å‹: {chain_type} | è¾“å…¥å‚æ•°: {self._format_inputs(inputs)}")

    def on_chain_end(self, outputs, **kwargs):
        duration = self._end_timer('chain')
        print(f"âœ… [{self._get_timestamp()}] é“¾æ‰§è¡Œå®Œæˆ | è€—æ—¶: {duration:.2f}s | è¾“å‡ºå‚æ•°: {self._format_outputs(outputs)}")

    def on_agent_action(self, action, **kwargs):
        print(f"ğŸ¤” [{self._get_timestamp()}] Agentå†³ç­– | é€‰æ‹©å·¥å…·: {action.tool} | è¾“å…¥: {action.tool_input[:200]}")

    def _start_timer(self, event_type):
        self.timers[event_type] = time.time()

    def _end_timer(self, event_type):
        return time.time() - self.timers.pop(event_type, time.time())

    def _get_timestamp(self):
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    def _format_inputs(self, inputs):
        return json.dumps(inputs, indent=2, ensure_ascii=False)[:300]

    def _format_outputs(self, outputs):
        return json.dumps(outputs, indent=2, ensure_ascii=False)[:300]

# 1. å®šä¹‰çŠ¶æ€æ¨¡å‹
class AgentState(TypedDict):
    messages: List[Any]  # å¯¹è¯å†å²
    task_status: str     # ä»»åŠ¡çŠ¶æ€
    task_plan: Dict     # ä»»åŠ¡è§„åˆ’
    execution_context: Dict  # æ‰§è¡Œä¸Šä¸‹æ–‡
    results: Dict       # æ‰§è¡Œç»“æœ        

# 2. å·¥å…·å®šä¹‰
@tools.tool
def browse_web(url: str) -> Dict:
    """æµè§ˆç½‘é¡µå¹¶æå–ä¿¡æ¯"""
    # å®é™…å®ç°éœ€è¦é›†æˆæµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·å¦‚Selenium
    return {"title": "ç¤ºä¾‹ç½‘é¡µ", "content": "ç½‘é¡µå†…å®¹æ‘˜è¦"}

@tools.tool
def edit_code(file_path: str, changes: Dict) -> Dict:
    """ç¼–è¾‘ä»£ç æ–‡ä»¶"""
    # å®é™…å®ç°éœ€è¦é›†æˆä»£ç ç¼–è¾‘å™¨API
    return {"status": "success", "file": file_path}

@tools.tool
def analyze_data(data: Dict, analysis_type: str) -> Dict:
    """åˆ†ææ•°æ®"""
    # å®é™…å®ç°éœ€è¦é›†æˆæ•°æ®åˆ†æåº“
    return {"analysis_result": "æ•°æ®åˆ†æç»“æœ"}

# 3. ä»»åŠ¡åˆ†è§£ä¸è§„åˆ’
def decompose_task(task_description: str) -> List[Dict]:
    """å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡"""
    # ä½¿ç”¨LLMåˆ†æä»»åŠ¡å¹¶ç”Ÿæˆå­ä»»åŠ¡åˆ—è¡¨
    return [
        {"id": 1, "name": "ä¿¡æ¯æ”¶é›†", "tools": ["browse_web"]},
        {"id": 2, "name": "æ•°æ®åˆ†æ", "tools": ["analyze_data"]},
        {"id": 3, "name": "ä»£ç å®ç°", "tools": ["edit_code"]}
    ]

def plan_execution(subtasks: List[Dict]) -> Dict:
    """è§„åˆ’ä»»åŠ¡æ‰§è¡Œé¡ºåº"""
    return {
        "execution_plan": subtasks,
        "dependencies": {"2": [1], "3": [2]}
    }

# 4. èŠ‚ç‚¹å‡½æ•°
def understand_request(state: AgentState) -> AgentState:
    """ç†è§£ç”¨æˆ·è¯·æ±‚å¹¶è§„åˆ’ä»»åŠ¡"""
    llm = ChatOpenAI(
        model=os.getenv("MODEL", "Qwen/Qwen2.5-Coder-32B-Instruct"),
        temperature=0.7,
        callbacks=[CustomCallbackHandler()]
    )
    messages = state["messages"]
    
    # åˆ†æç”¨æˆ·è¯·æ±‚
    response = llm.invoke([
        HumanMessage(content=f"åˆ†æä»¥ä¸‹ä»»åŠ¡éœ€æ±‚ï¼Œæå–å…³é”®ä¿¡æ¯å’Œç›®æ ‡ã€‚ç”¨æˆ·æ¶ˆæ¯: {messages[-1].content}")
    ])
    
    # åˆ†è§£ä»»åŠ¡
    subtasks = decompose_task(messages[-1].content)
    execution_plan = plan_execution(subtasks)
    
    return {
        **state,
        "task_plan": execution_plan,
        "task_status": "planned",
        "execution_context": {"current_subtask": 0}
    }

def execute_task(state: AgentState) -> AgentState:
    """æ‰§è¡Œä»»åŠ¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    context = state["execution_context"]
    plan = state["task_plan"]
    current_subtask = plan["execution_plan"][context["current_subtask"]]
    
    # åˆå§‹åŒ–å·¥å…·å®ä¾‹
    tools_map = {
        "browse_web": browse_web,
        "analyze_data": analyze_data,
        "edit_code": edit_code
    }
    
    # åˆå§‹åŒ–æ‰§è¡Œç»“æœå’Œæ—¥å¿—
    results = {}
    execution_log = context.get("logs", [])
    
    # æ‰§è¡Œå½“å‰å­ä»»åŠ¡
    for tool_name in current_subtask["tools"]:
        # è·å–å·¥å…·å®ä¾‹
        tool = tools_map.get(tool_name)
        if not tool:
            error_info = f"å·¥å…· {tool_name} æœªæ‰¾åˆ°"
            return {
                **state,
                "task_status": "error",
                "error_info": error_info
            }
        
        # å‡†å¤‡å·¥å…·è¾“å…¥å‚æ•°
        tool_input = {
            "browse_web": {"url": "https://example.com"},
            "analyze_data": {"data": results.get("web_data", {}), "analysis_type": "basic"},
            "edit_code": {"file_path": "example.py", "changes": {"line": 1, "content": "# New code"}}
        }.get(tool_name, {})
        
        try:
            # è®°å½•å·¥å…·è°ƒç”¨å¼€å§‹
            execution_log.append({
                "timestamp": datetime.now().isoformat(),
                "tool": tool_name,
                "status": "started",
                "input": tool_input
            })
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            result = tool.invoke(tool_input)
            
            # è®°å½•æˆåŠŸæ—¥å¿—
            execution_log.append({
                "timestamp": datetime.now().isoformat(),
                "tool": tool_name,
                "status": "success",
                "output": result
            })
            
            # ä¿å­˜ç»“æœ
            results[tool_name] = result
            
        except Exception as e:
            error_info = f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {str(e)}"
            return {
                **state,
                "task_status": "error",
                "error_info": error_info
            }
    
    # æ›´æ–°æ‰§è¡ŒçŠ¶æ€å’Œæ—¥å¿—
    context["current_subtask"] += 1
    context["logs"] = execution_log
    task_status = "completed" if context["current_subtask"] >= len(plan["execution_plan"]) else "executing"
    
    return {
        **state,
        "execution_context": context,
        "task_status": task_status,
        "results": {**state.get("results", {}), **results}
    }

def generate_report(state: AgentState) -> AgentState:
    """ç”Ÿæˆä»»åŠ¡æŠ¥å‘Š"""
    llm = ChatOpenAI(
        model=os.getenv("MODEL", "Qwen/Qwen2.5-Coder-32B-Instruct"),
        temperature=0.7,
        callbacks=[CustomCallbackHandler()]
    )
    
    results = state["results"]
    prompt = f"""
    åŸºäºä»¥ä¸‹æ‰§è¡Œç»“æœç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼š
    1. ç½‘é¡µæ•°æ®: {results.get('web_data', {})}
    2. åˆ†æç»“æœ: {results.get('analysis', {})}
    3. ä»£ç å˜æ›´: {results.get('code_changes', {})}
    
    è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æŠ¥å‘Šï¼š
    1. ä»»åŠ¡æ¦‚è¿°
    2. æ‰§è¡Œè¿‡ç¨‹
    3. å…³é”®å‘ç°
    4. å»ºè®®å’Œä¸‹ä¸€æ­¥
    """
    
    response = llm.invoke([HumanMessage(content=prompt)])
    
    messages = state["messages"] + [
        AIMessage(content=f"ä»»åŠ¡æ‰§è¡ŒæŠ¥å‘Š:\n\n{response.content}")
    ]
    
    return {
        **state,
        "messages": messages,
        "task_status": "completed"
    }

 
def handle_errors(state: AgentState) -> AgentState:
    """é”™è¯¯å¤„ç†å‡½æ•°
    åˆ†ææ‰§è¡Œé”™è¯¯å¹¶å°è¯•æ¢å¤
    """
    context = state["execution_context"]
    error_info = state.get("error_info", "æœªçŸ¥é”™è¯¯")
    
    # è®°å½•é”™è¯¯ä¿¡æ¯
    error_log = context.get("error_logs", [])
    error_log.append({
        "timestamp": datetime.now().isoformat(),
        "error": error_info
    })
    
    # ç®€å•çš„æ¢å¤ç­–ç•¥ï¼šé‡è¯•å½“å‰å¤±è´¥çš„å­ä»»åŠ¡
    if len(error_log) < 3:  # æœ€å¤šé‡è¯•3æ¬¡
        return {
            **state,
            "task_status": "executing",
            "execution_context": {
                **context,
                "error_logs": error_log,
                "retry_count": len(error_log)
            }
        }
    
    # è¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œæ ‡è®°ä¸ºæ— æ³•æ¢å¤
    return {
        **state,
        "task_status": "failed",
        "execution_context": {
            **context,
            "error_logs": error_log
        }
    }
   

# 5. æ¡ä»¶åˆ†æ”¯å¤„ç†
def should_end(state: AgentState) -> str:
    """å†³å®šå·¥ä½œæµç¨‹åº"""
    if state["task_status"] == "completed":
        return END
    if state["task_status"] == "planned":
        return "execute_task"
    if state["task_status"] == "executing":
        return "execute_task"
    return "understand_request"

# 6. æ„å»ºå·¥ä½œæµå›¾
# å®šä¹‰çŠ¶æ€è½¬æ¢æ¡ä»¶
def can_execute_task(state: AgentState) -> bool:
    """åˆ¤æ–­æ˜¯å¦å¯ä»¥æ‰§è¡Œä»»åŠ¡"""
    return state["task_status"] in ["planned", "executing"]

def is_task_completed(state: AgentState) -> bool:
    """åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å®Œæˆ"""
    return state["task_status"] == "completed"

def is_error_recovered(state: AgentState) -> bool:
    """åˆ¤æ–­é”™è¯¯æ˜¯å¦å·²æ¢å¤"""
    return state["task_status"] == "recovered"

# åˆ›å»ºå·¥ä½œæµå›¾å®ä¾‹
workflow = StateGraph(AgentState)

# æ·»åŠ æ ¸å¿ƒå¤„ç†èŠ‚ç‚¹
workflow.add_node("understand_request", understand_request)  # ç†è§£è¯·æ±‚
workflow.add_node("execute_task", execute_task)          # æ‰§è¡Œä»»åŠ¡
workflow.add_node("generate_report", generate_report)    # ç”ŸæˆæŠ¥å‘Š
workflow.add_node("error_handler", handle_errors)       # é”™è¯¯å¤„ç†

# è®¾ç½®å·¥ä½œæµèµ·ç‚¹
workflow.set_entry_point("understand_request")

# é…ç½®çŠ¶æ€è½¬æ¢è§„åˆ™
# 1. è¯·æ±‚ç†è§£èŠ‚ç‚¹çš„è½¬æ¢
workflow.add_conditional_edges(
    "understand_request",
    lambda state: "execute_task" if can_execute_task(state) else END,
    {
        "execute_task": "execute_task",  # å¯ä»¥æ‰§è¡Œåˆ™è½¬åˆ°æ‰§è¡ŒèŠ‚ç‚¹
        END: "generate_report"          # å¦åˆ™ç”ŸæˆæŠ¥å‘Šå¹¶ç»“æŸ
    }
)

# 2. ä»»åŠ¡æ‰§è¡ŒèŠ‚ç‚¹çš„è½¬æ¢
workflow.add_conditional_edges(
    "execute_task",
    lambda state: (
        "execute_task" if can_execute_task(state)
        else "error_handler" if state["task_status"] == "error"
        else "generate_report"
    ),
    {
        "execute_task": "execute_task",      # ç»§ç»­æ‰§è¡Œ
        "error_handler": "error_handler",    # é”™è¯¯å¤„ç†
        "generate_report": "generate_report"  # ç”ŸæˆæŠ¥å‘Š
    }
)

# 3. é”™è¯¯å¤„ç†èŠ‚ç‚¹çš„è½¬æ¢
workflow.add_conditional_edges(
    "error_handler",
    lambda state: "generate_report" if is_error_recovered(state) else END,
    {
        "generate_report": "generate_report",  # æ¢å¤åç”ŸæˆæŠ¥å‘Š
        END: END                             # æ— æ³•æ¢å¤åˆ™ç»“æŸ
    }
)

# 4. æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹è½¬æ¢
workflow.add_edge("generate_report", END)  # ç”ŸæˆæŠ¥å‘Šåç»“æŸ

# ç¼–è¯‘å·¥ä½œæµ
agent = workflow.compile()

def main():
    """ä¸»å‡½æ•°ï¼šåˆå§‹åŒ–å¹¶æ‰§è¡Œå·¥ä½œæµï¼Œå±•ç¤ºæ‰§è¡Œç»“æœ"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description='æ™ºèƒ½ä»»åŠ¡æ‰§è¡Œå·¥ä½œæµ')
    parser.add_argument('--task', type=str, help='è¦æ‰§è¡Œçš„ä»»åŠ¡æè¿°', default='è¯·å¸®æˆ‘åˆ¶å®šä¸€ä¸ªæ—…è¡Œè®¡åˆ’')
    args = parser.parse_args()

    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "messages": [HumanMessage(content=args.task)],
        "task_status": "new",
        "task_plan": {},
        "execution_context": {},
        "results": {}
    }

    # æ‰§è¡Œå·¥ä½œæµ
    print("\nğŸš€ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ...\n")
    result = agent.invoke(initial_state)

    # å±•ç¤ºæ‰§è¡Œç»“æœ
    print("\nğŸ“Š æ‰§è¡Œç»“æœæ±‡æ€»ï¼š")
    print("-" * 50)
    print(f"âœ… ä»»åŠ¡çŠ¶æ€ï¼š{result['task_status']}")
    
    if result.get('task_plan'):
        print("\nğŸ“‹ ä»»åŠ¡è§„åˆ’ï¼š")
        for task in result['task_plan'].get('execution_plan', []):
            print(f"  - {task['name']} (ID: {task['id']})")
    
    if result.get('results'):
        print("\nğŸ¯ æ‰§è¡Œç»“æœï¼š")
        for key, value in result['results'].items():
            print(f"  - {key}: {value}")
    
    if result.get('messages'):
        print("\nğŸ’¬ æœ€ç»ˆæŠ¥å‘Šï¼š")
        print(result['messages'][-1].content)

if __name__ == "__main__":
    main()

